{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5-MLaQkLNGF",
        "outputId": "5b7e02e4-556e-4192-f6e9-0ddb0ddd07c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.1/179.1 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.2/492.2 kB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.2/244.2 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.6/92.6 MB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.4/77.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m106.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m81.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.4/300.4 kB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.2/48.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m619.9/619.9 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m101.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 MB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.6/102.6 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.2/173.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.1/177.1 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.6/98.6 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.3/63.3 MB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.0/153.0 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m117.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for lit (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install -qU \\\n",
        "  transformers==4.31.0 \\\n",
        "  sentence-transformers==2.2.2 \\\n",
        "  pinecone-client==2.2.2 \\\n",
        "  datasets==2.14.0 \\\n",
        "  accelerate==0.21.0 \\\n",
        "  einops==0.6.1 \\\n",
        "  langchain==0.0.240 \\\n",
        "  xformers==0.0.20 \\\n",
        "  bitsandbytes==0.41.0 \\\n",
        "  peft==0.4.0 \\\n",
        "  trl==0.4.7 \\\n",
        "  accelerate==0.21.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Kne2apILoHt"
      },
      "outputs": [],
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n",
        "from transformers import TextStreamer\n",
        "import torch\n",
        "from transformers import pipeline, logging\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "\n",
        "model_id = 'meta-llama/Llama-2-70b-chat-hf'\n",
        "\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# set quantization configuration to load large model with less GPU memory\n",
        "# this requires the `bitsandbytes` library\n",
        "bnb_config = transformers.BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type='nf4',\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=bfloat16\n",
        ")\n",
        "\n",
        "# begin initializing HF items, need auth token for these\n",
        "hf_auth = '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'\n",
        "model_config = transformers.AutoConfig.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "\n",
        "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    trust_remote_code=True,\n",
        "    config=model_config,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    use_auth_token=hf_auth\n",
        ")\n",
        "model.eval()\n",
        "print(f\"Model loaded on {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TcT-Y9s8Mgvb"
      },
      "outputs": [],
      "source": [
        "tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
        "    model_id,\n",
        "    use_auth_token=hf_auth\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmwjHyq2MlVj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "\n",
        "    temperature=0.1,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwMedWMwxdiA",
        "outputId": "572ea938-805a-47b6-9ea7-d52c7f06357c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "Enter your question (type 'quit' to exit): quit\n",
            "\n",
            "Full Conversation:\n",
            "  hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation is all relate to 'Context 1'.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BO4kZE8HF2VS",
        "outputId": "468393ff-f07a-4bd0-ee77-2c1f6ff0094b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "Enter your question (type 'quit' to exit): i don't know\n",
            "Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)\n",
            "Enter your question (type 'quit' to exit): how many agent are there ?\n",
            "There are four main operating entities of Etiqa Insurance:\n",
            "\n",
            "1. Etiqa General Insurance Berhad\n",
            "2. Etiqa Life Insurance Berhad\n",
            "3. Etiqa General Takaful Berhad\n",
            "4. Etiqa Family Takaful Berhad\n",
            "\n",
            "Additionally, there are two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. Can you name one of the countries where Etiqa Insurance operates?\n",
            "\n",
            "(Note: I'll give you a hint, it starts with an \"S\".)\n",
            "Enter your question (type 'quit' to exit): singapore ?\n",
            "Yes, that's correct! Etiqa Insurance operates in Singapore. Well done!\n",
            "\n",
            "Now, let's move on to the next question. What is the name of the bancassurance partnership between Etiqa and Maybank?\n",
            "\n",
            "(Note: I'll give you a hint, it starts with an \"M\".)\n",
            "Enter your question (type 'quit' to exit): Mannnnnl ?\n",
            "I apologize, it seems you've provided an incorrect answer. The correct answer is \"Maybank\".\n",
            "\n",
            "Maybank is a bancassurance partnership between Etiqa and Maybank, which allows Maybank's customers to purchase Etiqa's insurance products and services through Maybank's branches and digital channels.\n",
            "\n",
            "Now, let's move on to the next question. What is the name of Etiqa's flagship product that offers comprehensive protection for individuals and families?\n",
            "\n",
            "(Note: I'll give you a hint, it starts with a \"P\".)\n",
            "Enter your question (type 'quit' to exit): btw, what is cat ?\n",
            "😺 Hello there, fellow feline friend! 😻\n",
            "\n",
            "CAT stands for \"Computer-Aided Translation.\" It's a software tool that helps translators with the translation process. It's like a superpowered dictionary that can translate text from one language to another with the help of a human translator.\n",
            "\n",
            "Think of it like this: CAT is like a robot that can help you with your homework, but you still need to tell it what to do and make sure it's doing it right. It's a useful tool, but it's not perfect, and it still needs a human touch to make sure the translation is accurate and natural-sounding.\n",
            "\n",
            "So, my furry friend, the next time you see someone using CAT, you'll know they're not talking about their pet cat, but about a cool tool that helps with translation! 😸💻\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-20c06926a2c0>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oTCsOPF_HpkL",
        "outputId": "74b13c47-cbe0-4779-8f24-a7d4d73a243d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi, nice to meet you \n",
            "Hello! It's great to meet you too. Are you here to learn about Etiqa Insurance? I'd be happy to help you with any questions you have. Let's start with the basics: what is Etiqa Insurance, and what services do they offer?\n",
            "Enter your question (type 'quit' to exit): yes, i don't know.\n",
            "Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They have a presence in several countries, including Malaysia, Singapore, Indonesia, the Philippines, and Cambodia.\n",
            "\n",
            "Now, let's break it down further. What is life insurance, and what does it cover?\n",
            "Enter your question (type 'quit' to exit): btw, what is cat ?\n",
            "Sure, I'd be happy to explain! \"Cat\" is an abbreviation for \"certification,\" which refers to a document or certificate that verifies an individual's competence or qualification in a particular field or profession. In the context of insurance, a cat often refers to a certified insurance professional who has completed a training program and passed an examination to demonstrate their knowledge and skills in the industry.\n",
            "\n",
            "Now, back to Etiqa Insurance. Can you tell me a little bit more about what you're interested in learning about the company? Are you looking for information on their products and services, their history and mission, or something else?\n",
            "Enter your question (type 'quit' to exit): i want to know total how many agents are they ?\n",
            "Sure, according to Etiqa Insurance's official website, they have a total of 17,000 agents across Malaysia, Singapore, Indonesia, the Philippines, and Cambodia. This number includes both life and general insurance agents.\n",
            "\n",
            "Now, let's move on to the next question. How many branches does Etiqa Insurance have in Malaysia?\n",
            "\n",
            "A) 10\n",
            "B) 20\n",
            "C) 30\n",
            "D) 40\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "Enter your question (type 'quit' to exit): 40 ?\n",
            "I apologize, but the answer is actually C) 30. Etiqa Insurance has 30 branches in Malaysia.\n",
            "\n",
            "Now, let's move on to the next question. What is the name of the takaful operator that Etiqa Insurance collaborates with in Malaysia?\n",
            "\n",
            "A) Takaful Malaysia\n",
            "B) Takaful Nasional\n",
            "C) Takaful Islamic\n",
            "D) Takaful Asia\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "Enter your question (type 'quit' to exit): a\n",
            "Sorry, that's incorrect. The answer is C) Takaful Islamic. Etiqa Insurance collaborates with Takaful Islamic in Malaysia.\n",
            "\n",
            "Here's the next question:\n",
            "\n",
            "What is the name of the life insurance product offered by Etiqa Insurance that provides coverage for death, terminal illness, and critical illness?\n",
            "\n",
            "A) i-Life\n",
            "B) LifeSaver\n",
            "C) LifeCare\n",
            "D) LifeGuard\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "Enter your question (type 'quit' to exit): not fun\n",
            "Sorry to hear that. Let's try to make it more fun!\n",
            "\n",
            "Here's a joke for you: Why did the scarecrow win an award?\n",
            "\n",
            "A) Because he was outstanding in his field!\n",
            "\n",
            "I hope that made you smile! Now, let's get back to the questions.\n",
            "\n",
            "What is the name of the life insurance product offered by Etiqa Insurance that provides coverage for death, terminal illness, and critical illness?\n",
            "\n",
            "A) i-Life\n",
            "B) LifeSaver\n",
            "C) LifeCare\n",
            "D) LifeGuard\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "Enter your question (type 'quit' to exit): i want to go next topic\n",
            "Sure, no problem! Let's move on to the next topic.\n",
            "\n",
            "What would you like to learn about next?\n",
            "\n",
            "You can choose from the following options:\n",
            "\n",
            "1. Etiqa Insurance's travel insurance products\n",
            "2. Etiqa Insurance's health insurance products\n",
            "3. Etiqa Insurance's savings and investment products\n",
            "4. Etiqa Insurance's customer service and claims process\n",
            "5. Etiqa Insurance's partners and collaborations\n",
            "\n",
            "Please select one of the options from the table above.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-aeac22b01542>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KmdmiZtM47d-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------------------------------------------------------------------\n",
        "  "
      ],
      "metadata": {
        "id": "ZlvUsfEE47xc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embed_model_id = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "#set the device\n",
        "device = f'cuda:{cuda.current_device()}' if cuda.is_available() else 'cpu'\n",
        "\n",
        "#load embed model\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embed_model_id,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594,
          "referenced_widgets": [
            "872c965ad6514865857dfd31afc6c9f8",
            "eb583d70344645d8bd57499630d4d611",
            "e6c49bb9a23744e1a36a342c30c6ad78",
            "9a89f5ee637e42a79bf1b540a7718037",
            "c726e041c4d04ee0a62382f061e58604",
            "90d45ccf01eb4c32812d58abe312e25e",
            "db9d93ff4bf64a9db54a0e7d70f1353f",
            "266acafdc1f2430b8fb45ec1a8fecef5",
            "15e3ab9a71364b469a4c17f5fb245833",
            "d340cd30452e4793b006e8214238442e",
            "22a4322eeaa2402ea41f2889b21f0a87",
            "14cb83ecfb504f2a9bf0e913e10b7864",
            "612a1f68fc8b4435b5c97dbc71ef3527",
            "7cc0a0f10f7a481ca845afbdc3ee891b",
            "8315aafb6e8f4b5eb394da51d10b8ac8",
            "511e61cf1afa426786ab133a086bcb39",
            "66949951b4b64ee988ecb16a2a57cb93",
            "bf558f6364394cfa825ff8004324b14e",
            "9dfa9e36646c4c80a9f6324c5b09a42f",
            "2abb1170d0aa49eb8021fe1d5bbe49c9",
            "4e427f4d5dcd4ad2a9496577f2f1b419",
            "7e05fdc6448448ffb5aacad690281a66",
            "0038d4b0b4244d0fa5cb312d4f5e254b",
            "c15c06b4e8af482cb5d65521c7a21247",
            "05a38fd6271640bc8d10b87109281aaa",
            "cefaa9e72c2a418cb0ad1ccb5975ac2b",
            "54c79b62a9b04be586a76012a104df67",
            "32474a81669240caae4eeeb5dadcd3c1",
            "85e3ccacfe3d4ddc8f7e0b0cb2bca3c5",
            "e77f8723b0ec49e5b472949e13688d0f",
            "8334e50165744ba8a9aeab62e9572a95",
            "650b7eb03fd44bbb85a71914dbf42abf",
            "e9da47b586df4887bf031d3c4de79fac",
            "15bab5935bd64b1aac22b5c6329c5c1c",
            "93cbbdb2794046cb92961979940543e5",
            "2e232432793f42c495e27c704f149bcd",
            "8c64a61f46d64667a23c6b401485611b",
            "42b580880391472f99ac2564c6ef0992",
            "21539b31ddc5490ab6c0b5b69786edba",
            "2078d57d5d6949368c1bcaefcb0d4ba3",
            "e3f7e88bcd334c12b132aa46236a7ace",
            "f8a26e45c9d544918209efe68277db9a",
            "4be4410b33ae4617babd405b4782e517",
            "75a358556a914b039c2e34bbb99a7b18",
            "e7f1e14297e843e189d82b48e4aedfb0",
            "88e73bf47a894f48b1a63ffa311638bb",
            "bcf10188e2024e319236fcdbbc5f2f65",
            "4620e2dc32124868a6efb536fe28037a",
            "cbdf142209ff44f7b52efb4a140bafc7",
            "696eb4efc1d94365aa165a52b572161b",
            "ae2c92be0fa54c58acbc0b3923250cb9",
            "4875ea6b060748e99a78d4e84f95f817",
            "43a65e384200459ebcb482a6dd42429f",
            "704b4bd1767b4409aedabbfafb5fc070",
            "3e67e688f63e4ce8aefec3a47484205b",
            "0181dc9eecb24c76907e731f9bcc580e",
            "2ff9c9c1c2f645f7b0ff083a9a3cffe3",
            "4f52f87634f3481fab4ae49e767c637f",
            "b45bcf72241d4ecdbece34d79b14386b",
            "f25b83cf9de94f8090585a851a32bda8",
            "840d2221d81f41828e03d409dde6c406",
            "e94cf58398484113a0e9b2cf7eec5263",
            "b153ffbe8e5b43dab785b5db14fb98e5",
            "b7be4f4c7bf8436e81e161ce5b227346",
            "56f8dd0b31c3479d99b745c193a93db1",
            "d40f1dc1cb454cc2a8409c9f06655f73",
            "d80e2f6b67a64175a4c44bd81cfb5d4e",
            "fee4a54c2ee94fe1beb88fed3b347b6c",
            "cf7986d1277b4689af19aa0bd340162c",
            "1ed95d229c544bd48c8aab9c7d94cbc4",
            "fc623c4097674bd6a8a98f3b88123263",
            "7a7b48e31088484e8940937d350d808a",
            "51eda076d73540f48f325c2db81c6d92",
            "0045cf87262244bd8f489855b60c9feb",
            "7043a50ac9af477b824acd9cffedd764",
            "8f3488c4fa044f6397437059b1217876",
            "0df574063501488a80196ccacbba6f2b",
            "239b13fd640b47c48a37fabe329d439f",
            "a5bce8245c3249089552832adedfa59c",
            "33ab83e6e0744ff7853bb43868b15d0e",
            "f3cd703267334dd4a176a27f0e917a63",
            "9088cabd91de4e0b9b53f2ddb7a2c32b",
            "dfca82b29f0845d5ab94d6e759a2e328",
            "c255757eab294c0e8af314505071fd02",
            "4bd93b6ab2904aaa9eac5af25b8ed8e0",
            "0f68a5134e4140bf933fe696bb46a085",
            "0b3c24c36abb422f8ca796408399ef1d",
            "16a47b599c034591970b121b86d41a12",
            "e898c85a5ae14c73b5fa8acb046e57d4",
            "94733e2ac50b460780c111eb7658b83e",
            "d30bc4a295ae4ad1a1ece802e3e81034",
            "ba5f5e49d0a4476f9b0addb128ac60fa",
            "1144fb80f7bd4fe5a829cb8bcb565df0",
            "e75a422c86764c1785f6f50292c31893",
            "4bd106ff719f470bbbfb0bf3d4dba486",
            "23fe8e7ba55643e4b3116e3c32e460c3",
            "5a856cfd54c7491ea3c1762c325f8763",
            "8f29ac1faf2f4aab8798ded3a83dc0c4",
            "20f775aee1b341cca1fbcd498cff3295",
            "dfcba1de4b124bea80110c415e5aa68d",
            "ab2ddc270d804867a16805ca50f0ecfb",
            "c589964ae1024278a50031df38e3ff09",
            "684049019c114d59b32ada9c7ea1d426",
            "92c70d2a6f0a4cec81578f4af0f26401",
            "bb61efe564014579acdfce1b0af3492a",
            "94baa031f525401487db92be4f5b2a3d",
            "2e791b3b78874c8eb627acc01206d707",
            "5b15660ba9d44f298f6e3ef9dd1ea979",
            "e0188d44ca53485cbe0904b040b52de4",
            "6fab13ab7e624ec9a1084956a647cdbe",
            "fa52b4de77a543528daac7bd94ed18fc",
            "1e98e622dd9547058327189de5509c42",
            "c2b704e6a0644aa49d34496c9cac8249",
            "c56448b81e474dc9937bca5db8f901ab",
            "2e1c78a7b1d54ac38f95d3e37a7a2554",
            "913b19e7619745babae69ff4e66e14b3",
            "2fc4dfbac4034be0a5860589ad54e4ae",
            "b95fb6860c544c4ca53ba152a63a755c",
            "824b395c752a48cfa25eca5378ddeea9",
            "dbe164990a3441bb9be622a8d71cea01",
            "3541861f76aa454ab4934bc246a22573",
            "bfa87df355b44111b531681cb5444a7c",
            "e6dc7b841b7249e7829f8bd1f3aded3c",
            "c3a013fb28d24da1a1abdb237a6333cc",
            "44e6f62b35344facaa0fedc4f226c5cd",
            "c6e6b4626e1c46c0b5cc76ac06200eb3",
            "6f4026f51c384d3985b8e788a0df638d",
            "fe9548efa1df4183b2545a183081fe23",
            "6acc7c4913a84c89ad8e3179e6d13003",
            "6ba7ce45df2e454cb86fe5b77693fc5c",
            "c649fc92198c4fb28a7a38d97cb8ea11",
            "ad322e7feb094bab8239d65b2efc4ae8",
            "c54d8ce7dcef40068ba74f8cf267155b",
            "4a82530ab9324be1856ea6d51e875717",
            "e127d73b00c343e4b41d7328c6cfc3e5",
            "06d7bb46ee294cdaadcf45ed50eca1a3",
            "d29e11a116a8488a82aed778f74f0101",
            "49f51550ebe44b628ff1dd3f767a4314",
            "3f88265860f4492ebf9f377723ce6ffb",
            "aee8eebd1b654e63871e257906881093",
            "15c0507971f74ae8ba377853a883f069",
            "76d09613b0854d9a9bf6f98eb418404d",
            "c37e93c82ee042fe81d0eaa0be65e5ab",
            "0dda4ebb5f2f4703a0072c950f369b4c",
            "30b9795471574190af4582495666d887",
            "d94f483318534126bc43380ca547bd79",
            "bf7f6d22e7204774ba5eef3bd909cdd0",
            "b039b1575fe74262a8a316d638d0ea00",
            "d4813310c4804b28b5f815fc78846e4c",
            "605adad882414d05ae0169bc4b5a751b",
            "6c9d3634c634457f983c01e64d6fa29d",
            "ad08b5cb43b44421bff4475f850fae42",
            "8e49bf4f991b47f3ade4440970b0ac85",
            "1a0dc50b55e547ad98e9b29c61ebe81d"
          ]
        },
        "id": "7XFOHmpj9O1n",
        "outputId": "83be4169-0f0b-4e02-b6c9-1a4d2a1f8d64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:72: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              ".gitattributes:   0%|          | 0.00/1.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "872c965ad6514865857dfd31afc6c9f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14cb83ecfb504f2a9bf0e913e10b7864"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0038d4b0b4244d0fa5cb312d4f5e254b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15bab5935bd64b1aac22b5c6329c5c1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7f1e14297e843e189d82b48e4aedfb0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0181dc9eecb24c76907e731f9bcc580e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d80e2f6b67a64175a4c44bd81cfb5d4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "239b13fd640b47c48a37fabe329d439f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e898c85a5ae14c73b5fa8acb046e57d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfcba1de4b124bea80110c415e5aa68d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa52b4de77a543528daac7bd94ed18fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfa87df355b44111b531681cb5444a7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c54d8ce7dcef40068ba74f8cf267155b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dda4ebb5f2f4703a0072c950f369b4c"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pinecone\n",
        "\n",
        "\n",
        "# get API key from app.pinecone.io and environment from console\n",
        "# pinecone is used to store the vectors\n",
        "pinecone.init(\n",
        "    api_key=os.environ.get('d736748c-6f4c-4987-820e-2a5a6911e251') or 'd736748c-6f4c-4987-820e-2a5a6911e251',\n",
        "    environment=os.environ.get('gcp-starter') or 'gcp-starter'\n",
        ")"
      ],
      "metadata": {
        "id": "hCJN7saK9dxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build the index template\n",
        "docs = [\n",
        "    \"this is one document\",\n",
        "    \"and another document\"\n",
        "]\n",
        "\n",
        "embeddings = embed_model.embed_documents(docs)\n",
        "\n",
        "print(f\"We have {len(embeddings)} doc embeddings, each with \"\n",
        "      f\"a dimensionality of {len(embeddings[0])}.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFiEUtxE9fee",
        "outputId": "5ceb6746-57b2-41f2-d74b-111cbd4683eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have 2 doc embeddings, each with a dimensionality of 384.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#initialize the index, create the index\n",
        "import time\n",
        "\n",
        "index_name = 'llama-2-rag'\n",
        "\n",
        "if index_name not in pinecone.list_indexes():\n",
        "    pinecone.create_index(\n",
        "        index_name,\n",
        "        dimension=len(embeddings[0]),\n",
        "        metric='cosine'\n",
        "    )\n",
        "    # wait for index to finish initialization\n",
        "    while not pinecone.describe_index(index_name).status['ready']:\n",
        "        time.sleep(1)"
      ],
      "metadata": {
        "id": "zaK-Dlzw9gvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#connect to the index\n",
        "index = pinecone.Index(index_name)\n",
        "index.describe_index_stats()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVPvZ7wE9iel",
        "outputId": "f39a5491-6081-478b-96bf-d1cbdbfda39e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'dimension': 384,\n",
              " 'index_fullness': 0.00018,\n",
              " 'namespaces': {'': {'vector_count': 18}},\n",
              " 'total_vector_count': 18}"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Features, Value\n",
        "\n",
        "# Define the features of the dataset\n",
        "features = Features({\n",
        "    'doi': Value('string'),\n",
        "    'chunk-id': Value('string'),\n",
        "    'chunk': Value('string'),\n",
        "    'source': Value('string'),\n",
        "    'title': Value('string')\n",
        "})\n",
        "\n",
        "# Create an empty dataset with these features\n",
        "dataset = Dataset.from_dict({feature: [] for feature in features}, features=features)"
      ],
      "metadata": {
        "id": "bV0P6_J29lB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import cuda, bfloat16\n",
        "import transformers\n"
      ],
      "metadata": {
        "id": "_hM_CGZ79l3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import LoraConfig, PeftModel\n",
        "from trl import SFTTrainer\n",
        "generate_text = transformers.pipeline(\n",
        "    model=model, tokenizer=tokenizer,\n",
        "    return_full_text=True,  # langchain expects the full text\n",
        "    task='text-generation',\n",
        "    # we pass model parameters here too\n",
        "\n",
        "    temperature=0.0,  # 'randomness' of outputs, 0.0 is the min and 1.0 the max\n",
        "    max_new_tokens=512,  # mex number of tokens to generate in the output\n",
        "    repetition_penalty=1.1  # without this output begins repeating\n",
        ")"
      ],
      "metadata": {
        "id": "bk8PbWzo9vyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "llm = HuggingFacePipeline(pipeline=generate_text)"
      ],
      "metadata": {
        "id": "uY1bptiJ9zn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = 'text'  # field in metadata that contains text content\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "OzT9tqKa94ZB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.vectorstores import Pinecone\n",
        "\n",
        "text_field = 'title'  # field in metadata that contains text content\n",
        "\n",
        "vectorstore = Pinecone(\n",
        "    index, embed_model.embed_query, text_field\n",
        ")"
      ],
      "metadata": {
        "id": "4ceuxXRQPEr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = 'What is ANGeL and how do you access it?'\n",
        "\n",
        "vectorstore.similarity_search(\n",
        "    query,  # the search query\n",
        "    k=3  # returns top 3 most relevant chunks of text\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wiIkLf8955p",
        "outputId": "540dba28-64b5-4cfd-b30a-2cb169e00127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(page_content='The Etiqa\\'s support system called ANGeL, which stands for Attractive New Generation e-Learning. ANGeL is described as a web and mobile learning management system that facilitates product learning. Users are prompted to register for access, with the steps for registration detailed in an appendix not visible in the image. The website for registration is provided: https://lift.angelms.com.my. The completion of the e-learning module is required before the end of today\\'s session to enable product learning. The visual component of the image features a sign-in page with vibrant, colorful graphics and an avatar labeled \"ANGeL my Learning Companion\" that greets the users with a \"HELLO\". This suggests a user-friendly interface aimed at enhancing the learning experience for Etiqa\\'s partners or employees.', metadata={'source': '-', 'title': \"Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\"}),\n",
              " Document(page_content=\"The Etiqa Partner Portal is a secure online system provided by Etiqa Insurance & Takaful for authorized individuals. It's designed for partners to log in and manage activities related to their association with Etiqa. The portal emphasizes security and privacy, requiring users to set up a security image and phrase for verification during login, and it sends a verification link to the user's email to activate the login security setup. Additionally, the portal has features for password reset and security question setup to aid users in maintaining the security of their accounts. The portal warns that its use is restricted to individuals and activities authorized by the management of Etiqa Insurance & Takaful, and unauthorized use may result in disciplinary action and/or legal prosecution. Moreover, 'My Etiqa' is another portal that allows users to manage their policies and certificates anytime and anywhere, indicating that Etiqa provides various online platforms tailored to different user needs. For more detailed information or to access the portal, you can visit the official Etiqa Partner Portal website.\", metadata={'source': '-', 'title': 'Securing Synergy: The Etiqa Partner Portal Experience'}),\n",
              " Document(page_content=\"The Etiqa's aspirations for the year 2023. The company's goal is to make the world a better place by prioritizing the interests of customers and communities. They emphasize providing protection and wellness offerings to as many people as possible. Their approach includes delivering advice that puts the customer's interest first, creating a fast and easy customer experience, and driving technology across the organization. Additionally, there is a focus on retaining only highly effective people within their teams. The graphic elements reinforce the theme of speed, customer focus, technological advancement, and effective personnel, all of which are part of Etiqa's vision for enhancing their service and outreach.\", metadata={'source': '-', 'title': 'Etiqa 2023: Charting a Future of Customer-Centric Growth and Technological Innovation'})]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "rag_pipeline = RetrievalQA.from_chain_type(\n",
        "    llm=llm, chain_type='stuff',\n",
        "    retriever=vectorstore.as_retriever()\n",
        ")"
      ],
      "metadata": {
        "id": "3V1xVkZ597c1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "from langchain import PromptTemplate,  LLMChain\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from transformers import pipeline, logging\n",
        "pipe = pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 2000,\n",
        "                do_sample=True,\n",
        "                top_k=5,\n",
        "                top_p = 0.9,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "llm = HuggingFacePipeline(pipeline = pipe, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "VRRhTKFj99UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oKplyxUg9_NL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot that can determine which topics the user has not been taught.\n",
        "    The condition determines the topics that have already been taught :\n",
        "    1.User has understood the topic.\n",
        "    2.Which answers the chatbot has already provided.\n",
        "    3.the user want to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "    List down the topic has not been taught :\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "id": "Pl6jKgAR4tNp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4dd06314-cffb-4316-a00e-116fd1da89e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot that can determine which topics the user has not been taught.\n",
            "    The condition determines the topics that have already been taught :\n",
            "    1.User has understood the topic.\n",
            "    2.Which answers the chatbot has already provided.\n",
            "    3.the user want to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List down the topic has not been taught :\n",
            "    [/INST]\n",
            "     Based on the conversation provided, it appears that the user has not been taught the topic of \"Etiqa history\". Therefore, the topic that has not been taught is:\n",
            "\n",
            "1. Etiqa history\n",
            "Enter your question (type 'quit' to exit): i don't know\n",
            "Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot that can determine which topics the user has not been taught.\n",
            "    The condition determines the topics that have already been taught :\n",
            "    1.User has understood the topic.\n",
            "    2.Which answers the chatbot has already provided.\n",
            "    3.the user want to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]i don't know [/INST] <</SYS>>Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List down the topic has not been taught :\n",
            "    [/INST]\n",
            "    1. The main difference between life insurance and general insurance.\n",
            "2. The history of Etiqa Insurance.\n",
            "\n",
            "These are the two topics that have not been taught in the conversation so far.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-1d09c92b0228>\u001b[0m in \u001b[0;36m<cell line: 32>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "    List the topics that have not been taught, based on the given topic :\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3wQ57WAqBgME",
        "outputId": "f4d0c8f2-b6d0-4ec0-87fa-5aa0758fc67a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List the topics that have not been taught, based on the given topic :\n",
            "    [/INST]\n",
            "     Based on the given conversation, the following topics have not been taught yet:\n",
            "\n",
            "1. Etiqa history\n",
            "\n",
            "The conversation only touched on Etiqa Insurance, and did not cover Etiqa history. Therefore, Etiqa history is a topic that has not been taught yet.\n",
            "Enter your question (type 'quit' to exit): i don't know\n",
            "Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]i don't know [/INST] <</SYS>>Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List the topics that have not been taught, based on the given topic :\n",
            "    [/INST]\n",
            "    1. Etiqa history\n",
            "\n",
            "The conversation has not touched on the topic of Etiqa history, which is a subtopic under Etiqa Insurance. Therefore, this topic can be considered as one that has not been taught.\n",
            "Enter your question (type 'quit' to exit): quit\n",
            "\n",
            "Full Conversation:\n",
            "  hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]i don't know [/INST] <</SYS>>Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's move on to the next question. What is the main difference between life insurance and general insurance?\n",
            "\n",
            "(Note: I'll give you a hint, it has something to do with the type of coverage provided.)</s><s>[INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "    List the topics that have not been taught, based on the given topic :\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "    # Finding the last occurrence of [/INST]\n",
        "    last_inst_index = generated_text2.rfind(\"[/INST]\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of [/INST]\n",
        "    if last_inst_index != -1:\n",
        "        text_after_last_inst = generated_text2[last_inst_index + len(\"[/INST]\"):]\n",
        "        output = re.split(r'\\d+\\.', text_after_last_inst)\n",
        "        # Remove empty strings and strip whitespace\n",
        "        output = [item.strip() for item in output if item.strip()]\n",
        "    else:\n",
        "        output = []\n",
        "\n",
        "    print(\"output : \")\n",
        "    print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 956
        },
        "id": "K3gWcdIgDNmw",
        "outputId": "f4d47998-0168-4c40-cbe0-f07879568315"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List the topics that have not been taught, based on the given topic :\n",
            "    [/INST]\n",
            "     Based on the given conversation, the following topics have not been taught yet:\n",
            "\n",
            "1. Etiqa history\n",
            "\n",
            "The conversation only touched on Etiqa Insurance, and did not cover Etiqa history. Therefore, Etiqa history is a topic that has not been taught yet.\n",
            "output : \n",
            "['Based on the given conversation, the following topics have not been taught yet:', 'Etiqa history\\n\\nThe conversation only touched on Etiqa Insurance, and did not cover Etiqa history. Therefore, Etiqa history is a topic that has not been taught yet.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-2b7693c5b5f8>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "    List the topics that have not been taught, based on the given topic :\n",
        "    [/INST]\n",
        "    1.\n",
        "    2.\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "    # Finding the last occurrence of [/INST]\n",
        "    last_inst_index = generated_text2.rfind(\"[/INST]\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of [/INST]\n",
        "    if last_inst_index != -1:\n",
        "        text_after_last_inst = generated_text2[last_inst_index + len(\"[/INST]\"):]\n",
        "        output = re.split(r'\\d+\\.', text_after_last_inst)\n",
        "        # Remove empty strings and strip whitespace\n",
        "        output = [item.strip() for item in output if item.strip()]\n",
        "    else:\n",
        "        output = []\n",
        "\n",
        "    print(\"output : \")\n",
        "    print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wHFMm5I-EGbW",
        "outputId": "ed090a2d-2d8c-40fa-8680-f2c69b4fcf3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet learned. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    List the topics that have not been taught, based on the given topic :\n",
            "    [/INST]\n",
            "    1.\n",
            "    2.\n",
            "    3.\n",
            "\n",
            "The topics that have not been taught are:\n",
            "\n",
            "1. Etiqa Insurance\n",
            "2. Etiqa history\n",
            "\n",
            "The conversation has not covered these topics yet, and the user has not indicated that they have already understood these topics or prefer to skip them. Therefore, these topics are still open for discussion.\n",
            "output : \n",
            "['The topics that have not been taught are:', 'Etiqa Insurance', 'Etiqa history\\n\\nThe conversation has not covered these topics yet, and the user has not indicated that they have already understood these topics or prefer to skip them. Therefore, these topics are still open for discussion.']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-a79792704af7>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"Enter your question (type 'quit' to exit): \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {topic[0]['title']}:\n",
        "    {topic[0]['context']}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    [/INST]\n",
        "    List the topics that user has not yet understood, based on the given topic :\n",
        "    1.\n",
        "    2.\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "    # Finding the last occurrence of [/INST]\n",
        "    last_inst_index = generated_text2.rfind(\"[/INST]\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of [/INST]\n",
        "    if last_inst_index != -1:\n",
        "        text_after_last_inst = generated_text2[last_inst_index + len(\"[/INST]\"):]\n",
        "        output = re.split(r'\\d+\\.', text_after_last_inst)\n",
        "        # Remove empty strings and strip whitespace\n",
        "        output = [item.strip() for item in output if item.strip()]\n",
        "    else:\n",
        "        output = []\n",
        "\n",
        "    print(\"output : \")\n",
        "    print(output)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O4hsoUJwEeop",
        "outputId": "4d5b485e-12f5-4001-ba9c-f16fb45e3f94"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question (type 'quit' to exit): hi\n",
            "Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    \n",
            "    [/INST]\n",
            "    List the topics that user has not yet understood, based on the given topic :\n",
            "    1.\n",
            "    2.\n",
            "    3.\n",
            "\n",
            "The user has not yet understood the following topics :\n",
            "\n",
            "1. Etiqa Insurance\n",
            "2. Etiqa history\n",
            "\n",
            "Therefore, the topics that the user has not yet understood are :\n",
            "\n",
            "1. Etiqa Insurance\n",
            "2. Etiqa history\n",
            "output : \n",
            "['List the topics that user has not yet understood, based on the given topic :', 'The user has not yet understood the following topics :', 'Etiqa Insurance', 'Etiqa history\\n\\nTherefore, the topics that the user has not yet understood are :', 'Etiqa Insurance', 'Etiqa history']\n",
            "Enter your question (type 'quit' to exit): i dont know\n",
            "Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's start with some basic questions. What is life insurance?\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation. The topics are:\n",
            "    1.Etiqa Insurance\n",
            "    2.Etiqa history\n",
            "    here is the conversation :\n",
            "     hi [/INST] <</SYS>>Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you learn more about this topic. Let's get started!\n",
            "\n",
            "First, can you tell me what Etiqa Insurance is?</s><s>[INST]i dont know [/INST] <</SYS>>Sure, no problem! Etiqa Insurance is an insurance company that offers various types of insurance policies, including life insurance, general insurance, and takaful plans. They operate in several countries across ASEAN, including Malaysia, Singapore, Indonesia, Philippines, and Cambodia.\n",
            "\n",
            "Now, let's start with some basic questions. What is life insurance?</s><s>[INST]\n",
            "\n",
            "\n",
            "    <</SYS>>\n",
            "    \n",
            "    [/INST]\n",
            "    List the topics that user has not yet understood, based on the given topic :\n",
            "    1.\n",
            "    2.\n",
            "    3.\n",
            "\n",
            "(Note: The user has not yet understood the topic of Etiqa Insurance)\n",
            "\n",
            "Based on the conversation, the user has not yet understood the following topics:\n",
            "\n",
            "1. Etiqa Insurance\n",
            "2. Etiqa history\n",
            "\n",
            "Therefore, the list of topics that the user has not yet understood is:\n",
            "\n",
            "1. Etiqa Insurance\n",
            "2. Etiqa history\n",
            "output : \n",
            "['List the topics that user has not yet understood, based on the given topic :', '(Note: The user has not yet understood the topic of Etiqa Insurance)\\n\\nBased on the conversation, the user has not yet understood the following topics:', 'Etiqa Insurance', 'Etiqa history\\n\\nTherefore, the list of topics that the user has not yet understood is:', 'Etiqa Insurance', 'Etiqa history']\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-99cc73c24799>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter your question (type 'quit' to exit): \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST] \"\n",
        "    # Prepare the prompt\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {matches[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      matches[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation. The topics are:\n",
        "    1.{topic[0]['title']}\n",
        "    2.{topic[1]['title']}\n",
        "    here is the conversation :\n",
        "    {history}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    [/INST]\n",
        "    No need explaination and extra things just provided the final answers.\n",
        "    List the topics that user has not yet understood, based on the given topic :\n",
        "    *1.\n",
        "    *2.\n",
        "    \"\"\"\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "    # Finding the last occurrence of the pattern \"*1.\"\n",
        "    last_index = generated_text2.rfind(\"*1.\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of \"*1.\"\n",
        "    if last_index != -1:\n",
        "        text_after_last_occurrence = generated_text2[last_index:]\n",
        "        # Matching lines starting with \"*1.\", \"*2.\", etc., until the end of the string or until a line that doesn't start with \"*\"\n",
        "        pattern = r\"(\\*\\d+\\..+?)(?=\\*\\d+\\.|$)\"\n",
        "        matches = re.findall(pattern, text_after_last_occurrence, re.DOTALL)\n",
        "        output = \"\\n\".join(matches).strip()\n",
        "    else:\n",
        "        output = \"\"\n",
        "\n",
        "    output\n",
        "\n",
        "\n",
        "\n",
        "    print(\"output : \")\n",
        "    print(output)\n",
        "    pattern = r\"\\*\\d+\\.\\s*(.+)\"\n",
        "\n",
        "    # Finding matches\n",
        "    matches = re.findall(pattern, output)\n",
        "\n",
        "\n",
        "\n",
        "    print(\"matches : \")\n",
        "    print(matches[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "jtmPCcttF9yp",
        "outputId": "dc4c8cb4-148f-43aa-ad6f-cd71b18c6977"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "User : hi\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-d97f9bef11d0>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;34m*\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m     \"\"\"\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mresult2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mgenerated_text2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum :\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    694\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemv_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m ):\n\u001b[0;32m-> 1477\u001b[0;31m     \u001b[0mprev_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m     \u001b[0;31m#sout = check_matmul(A, B, out, transposed_A, transposed_B, expected_type=A.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mpre_call\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mprev_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprev_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    # Prepare the prompt\n",
        "    prompt2 = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
        "    1.The user has understood the topic.\n",
        "    2.The answers the chatbot has already provided on the topic.\n",
        "    3.The user's preference to skip that topic.\n",
        "    Based on the provided conversation :\n",
        "    User : hi\\n\n",
        "    The topics are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "    <</SYS>>\n",
        "\n",
        "\n",
        "    No need explaination and extra things just provided the final answers.\n",
        "    List the topics that user has not yet understood in below, based on the given topic :\n",
        "    *1.\n",
        "    *2.\n",
        "    *3.\n",
        "    *4.\n",
        "    *5.\n",
        "    *6.\n",
        "    [/INST]\n",
        "    \"\"\"\n",
        "    print(\"11111111:\")\n",
        "    print(history)\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "    # Finding the last occurrence of the pattern \"* 1.\"\n",
        "    last_index = generated_text2.rfind(\"* 1.\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of \"* 1.\"\n",
        "    if last_index != -1:\n",
        "        text_after_last_occurrence = generated_text2[last_index:]\n",
        "        # Matching lines starting with \"* 1.\", \"* 2.\", etc., until the end of the string or until a line that doesn't start with \"*\"\n",
        "        pattern = r\"(\\*\\s\\d+\\..+?)(?=\\*\\s\\d+\\.|$)\"\n",
        "        matches = re.findall(pattern, text_after_last_occurrence, re.DOTALL)\n",
        "        output = \"\\n\".join(matches).strip()\n",
        "    else:\n",
        "        output = \"\"\n",
        "\n",
        "    # Extracting just the topics from the matches\n",
        "    pattern = r\"\\*\\s\\d+\\.\\s*(.+)\"\n",
        "\n",
        "    # Finding matches\n",
        "    topics = re.findall(pattern, output)\n",
        "\n",
        "    print(\"Output:\")\n",
        "    print(output)\n",
        "    print(\"\\nExtracted Topics:\")\n",
        "    print(topics)\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {matches[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      matches[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"matches : \")\n",
        "    print(matches[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "klISOpX_Oq_l",
        "outputId": "0467e9ef-4fbb-48b5-c07d-0df4ed386ca6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : i\n",
            "11111111:\n",
            " i [/INST]\n",
            "sum :\n",
            "<s>[INST]<<SYS>>\n",
            "    You are a summarizing bot capable of identifying topics that the user has not yet understood. The criteria for determining the topics that have already been covered include :\n",
            "    1.The user has understood the topic.\n",
            "    2.The answers the chatbot has already provided on the topic.\n",
            "    3.The user's preference to skip that topic.\n",
            "    Based on the provided conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "    <</SYS>>\n",
            "    \n",
            "    \n",
            "    No need explaination and extra things just provided the final answers.\n",
            "    List the topics that user has not yet understood in below, based on the given topic :\n",
            "    *1.\n",
            "    *2.\n",
            "    *3.\n",
            "    *4.\n",
            "    *5.\n",
            "    *6.\n",
            "    [/INST]\n",
            "     Based on the provided conversation, the user has not yet understood the following topics:\n",
            "\n",
            "* 2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "* 3. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "* 4. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "* 5. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "* 6. Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "Output:\n",
            "\n",
            "\n",
            "Extracted Topics:\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-50-b3d2a150283e>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m     \u001b[0mContext\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m       {vectorstore.similarity_search(\n\u001b[1;32m    120\u001b[0m       \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    Conversation :\n",
        "    User : hi\\n Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\\n\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics and chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "    # Finding the last occurrence of the pattern \"* 1.\"\n",
        "    last_index = generated_text2.rfind(\"* 1.\")\n",
        "\n",
        "    # Extracting the text after the last occurrence of \"* 1.\"\n",
        "    if last_index != -1:\n",
        "        text_after_last_occurrence = generated_text2[last_index:]\n",
        "        # Matching lines starting with \"* 1.\", \"* 2.\", etc., until the end of the string or until a line that doesn't start with \"*\"\n",
        "        pattern = r\"(\\*\\s\\d+\\..+?)(?=\\*\\s\\d+\\.|$)\"\n",
        "        matches = re.findall(pattern, text_after_last_occurrence, re.DOTALL)\n",
        "        output = \"\\n\".join(matches).strip()\n",
        "    else:\n",
        "        output = \"\"\n",
        "\n",
        "    # Extracting just the topics from the matches\n",
        "    pattern = r\"\\*\\s\\d+\\.\\s*(.+)\"\n",
        "\n",
        "    # Finding matches\n",
        "    topics = re.findall(pattern, output)\n",
        "\n",
        "    print(\"Output:\")\n",
        "    print(output)\n",
        "    print(\"\\nExtracted Topics:\")\n",
        "    print(topics)\n",
        "\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {matches[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      matches[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" <</SYS>>{response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(\"matches : \")\n",
        "    print(matches[0])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 914
        },
        "id": "i4PLH0GLZXMI",
        "outputId": "3c6114ad-84d5-4436-8c64-17ab445598d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            " Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration (not mentioned)\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 (not mentioned)\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 (not mentioned)\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence (not mentioned)\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact (not mentioned)\n",
            "\n",
            "The user has only responded to the greeting and has not expressed any interest in any of the topics. Therefore, all topics are considered not understood and skipped.\n",
            "Output:\n",
            "\n",
            "\n",
            "Extracted Topics:\n",
            "[]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-54-5113413592da>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m     \u001b[0mContext\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m       {vectorstore.similarity_search(\n\u001b[1;32m    112\u001b[0m       \u001b[0mmatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: list index out of range"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    Conversation :\n",
        "    User : hi\\n Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\\n\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics and chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"not understood Topics and chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmjQKvfidyEZ",
        "outputId": "8332e19f-d7f0-4713-9d79-a603be93b00a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            " Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration (not mentioned)\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 (not mentioned)\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 (not mentioned)\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence (not mentioned)\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact (not mentioned)\n",
            "\n",
            "The user has only responded to the greeting and has not expressed any interest in any of the topics. Therefore, all topics are considered not understood and skipped.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "User : quit\n",
            "\n",
            "Full Conversation:\n",
            "  hi [/INST] Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's holistic pathway for growth and success. Let's start by discussing the beliefs that guide Etiqa's approach.\n",
            "\n",
            "Based on the provided context, what do you think are the main beliefs that drive Etiqa's vision for customer-centric growth and technological innovation?</s><s>[INST]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = \"\"\n",
        "\n",
        "    for conversation in history:\n",
        "        if \"[/INST]\" in conversation:\n",
        "            # Splitting and cleaning the conversation parts\n",
        "            user_part, chatbot_part = conversation.split(\"[/INST]\", 1)\n",
        "            user_part = user_part.strip()\n",
        "            chatbot_part = chatbot_part.strip(\"<s><s>[INST]\").strip()\n",
        "\n",
        "            # Formatting and adding to the single string\n",
        "            formatted_conversation = f\"User : {user_part}\\nChatbot : {chatbot_part}\\n\\n---\\n\"\n",
        "        else:\n",
        "            formatted_conversation = \"Delimiter not found in conversation.\\n\\n---\\n\"\n",
        "\n",
        "        formatted_conversations += formatted_conversation\n",
        "\n",
        "    formatted_conversations\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    Conversation :\n",
        "    User : hi\\n Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\\n\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics and chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"not understood Topics and chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b8vLD8xPf72H",
        "outputId": "5cc12712-6f9b-4749-a867-6186def15d6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            " Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration (not mentioned)\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 (not mentioned)\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 (not mentioned)\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence (not mentioned)\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact (not mentioned)\n",
            "\n",
            "The user has only responded to the greeting and has not expressed any interest in any of the topics. Therefore, all topics are considered not understood and skipped.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "User : i don't know\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "Delimiter not found in conversation.\n",
            "\n",
            "---\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            " Chatbot : Hi, good morning today we need to learn about Etiqa's Holistic Pathway for Growth and Success are you interest in it ?\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration (not mentioned)\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 (not mentioned)\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 (not mentioned)\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence (not mentioned)\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact (not mentioned)\n",
            "\n",
            "The user has only responded to the greeting and has not expressed any interest in any of the topics. Therefore, all topics are considered not understood and skipped.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (mentioned in the conversation)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-61-0b26ef3b4966>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    694\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mk_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0mquery_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbsz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_forward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mnew_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mpre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    288\u001b[0m                 )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 290\u001b[0;31m         return send_to_device(args, self.execution_device), send_to_device(\n\u001b[0m\u001b[1;32m    291\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecution_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \"\"\"\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 151\u001b[0;31m         return honor_type(\n\u001b[0m\u001b[1;32m    152\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mhonor_type\u001b[0;34m(obj, generator)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         return honor_type(\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         )\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msend_to_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnon_blocking\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         )\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mskip_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mskip_keys\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/typing.py\u001b[0m in \u001b[0;36m__instancecheck__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0msetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__origin__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 993\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__instancecheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__subclasscheck__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reformat_conversation(conversation):\n",
        "    # Splitting the conversation by [/INST] and <s>[INST]\n",
        "    parts = re.split(r'\\[/INST\\]|<s>\\[INST\\]', conversation)\n",
        "\n",
        "    # Initializing formatted conversation\n",
        "    formatted_conversation = \"\"\n",
        "\n",
        "    # Assigning roles (User or Chatbot) to each part\n",
        "    for i, part in enumerate(parts):\n",
        "        if i % 2 == 0:  # Even index, User's part\n",
        "            role = \"User\"\n",
        "        else:  # Odd index, Chatbot's part\n",
        "            role = \"Chatbot\"\n",
        "\n",
        "        # Cleaning and adding the part to formatted conversation\n",
        "        part = part.strip().replace(\"</s>\", \"\").replace(\"<s>\", \"\")\n",
        "        if part:  # Only add non-empty parts\n",
        "            formatted_conversation += f\"{role} : {part}\\n\"\n",
        "\n",
        "    return formatted_conversation"
      ],
      "metadata": {
        "id": "Fs8YXelBjLWI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example input strings\n",
        "input1 = \"hi [/INST]\"\n",
        "input2 = \"\"\"\n",
        "hi [/INST] Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's holistic pathway for growth and success. Let's start by discussing the beliefs that guide Etiqa's approach.\n",
        "\n",
        "Based on the provided context, what do you think are the main beliefs that drive Etiqa's vision for customer-centric growth and technological innovation?</s><s>[INST] sure [/INST] nicee </s><s>[INST]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Reformatting the conversations\n",
        "formatted_conversation1 = reformat_conversation(input1)\n",
        "formatted_conversation2 = reformat_conversation(input2)\n",
        "\n",
        "print(formatted_conversation2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-3imyHgjXe2",
        "outputId": "add06081-6470-457b-8e1c-7b02c5267891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's holistic pathway for growth and success. Let's start by discussing the beliefs that guide Etiqa's approach.\n",
            "\n",
            "Based on the provided context, what do you think are the main beliefs that drive Etiqa's vision for customer-centric growth and technological innovation?\n",
            "User : sure\n",
            "Chatbot : nicee \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics and chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"not understood Topics and chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_ATMpsDojOqE",
        "outputId": "522dfb00-82d2-440e-976b-14ef041dc667"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not expressed any interest in any of the topics, and has not provided any feedback or questions related to the topics. Therefore, it can be inferred that the user has not understood any of the topics yet.\n",
            "\n",
            "However, it's worth noting that the user has only said \"hi\" in the conversation, which doesn't provide much information about their level of understanding or interest in the topics. Further conversation and feedback from the user would be necessary to determine their level of understanding and interest in the topics.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not understood any of the topics yet as the conversation just started, and they have expressed a desire to skip all the topics listed.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i also don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i also don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i also don't know\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not understood any of the topics yet as the conversation just started, and they have expressed a desire to skip all the topics listed.\n",
            "\n",
            "Therefore, the topics that the user has already understood are none, and the topics they have chosen to skip are all six topics listed.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i want to skip this topic\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i also don't know\n",
            "Chatbot : Sure, no problem! 'Customer-centric growth' means that Etiqa wants to focus on the needs and well-being of their customers. They want to provide their customers with the best possible experience and make sure that they are satisfied with their services.\n",
            "\n",
            "To achieve this, Etiqa might take various approaches, such as:\n",
            "\n",
            "* Listening to customer feedback and using it to improve their services\n",
            "* Offering personalized services that cater to individual customer needs\n",
            "* Providing transparent and easily accessible information about their services\n",
            "* Offering competitive pricing and promotions that benefit their customers\n",
            "\n",
            "By taking these steps, Etiqa aims to build trust and loyalty with their customers, which in turn can lead to growth and success for the company.\n",
            "\n",
            "Now, let's move on to the next topic. What do you think Etiqa means by \"technological innovation\"? Can you give me your interpretation?\n",
            "User : i want to skip this topic\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i also don't know\n",
            "Chatbot : Sure, no problem! 'Customer-centric growth' means that Etiqa wants to focus on the needs and well-being of their customers. They want to provide their customers with the best possible experience and make sure that they are satisfied with their services.\n",
            "\n",
            "To achieve this, Etiqa might take various approaches, such as:\n",
            "\n",
            "* Listening to customer feedback and using it to improve their services\n",
            "* Offering personalized services that cater to individual customer needs\n",
            "* Providing transparent and easily accessible information about their services\n",
            "* Offering competitive pricing and promotions that benefit their customers\n",
            "\n",
            "By taking these steps, Etiqa aims to build trust and loyalty with their customers, which in turn can lead to growth and success for the company.\n",
            "\n",
            "Now, let's move on to the next topic. What do you think Etiqa means by \"technological innovation\"? Can you give me your interpretation?\n",
            "User : i want to skip this topic\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics and chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Understood Topics:\n",
            "\n",
            "1.Etiqa's aspirations for 2023\n",
            "2.Etiqa's approach to customer-centric growth\n",
            "3.Etiqa's meaning of \"customer-centric growth\"\n",
            "4.Etiqa's approach to technological innovation\n",
            "\n",
            "The user has expressed a desire to skip the following topics:\n",
            "\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Therefore, the topics that the user has already understood are:\n",
            "\n",
            "1.Etiqa's aspirations for 2023\n",
            "2.Etiqa's approach to customer-centric growth\n",
            "3.Etiqa's meaning of \"customer-centric growth\"\n",
            "4.Etiqa's approach to technological innovation\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-67-59a527216e2e>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics or not chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"not understood Topics or not chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "60WTRdJAlgoF",
        "outputId": "e3742851-fdcc-461e-b593-8459ed7b990b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not expressed any desire to skip any of the topics, and the conversation is just a simple greeting, so it can be inferred that the user has not understood any of the topics yet.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i don' know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don' know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don' know\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not shown any understanding or interest in any of the topics listed above, as they have not provided any information or asked any questions related to these topics. Therefore, it can be inferred that the user has not understood these topics or chosen to skip them.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i want to skip this topic\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don' know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i want to skip this topic\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don' know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : i want to skip this topic\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Topics the user has understood or chosen to skip:\n",
            "\n",
            "1.Etiqa's aspirations for 2023\n",
            "2.Etiqa's approach to customer-centric growth\n",
            "\n",
            "The user has not expressed any desire to skip the topic of Etiqa's aspirations for 2023, and the chatbot has provided information on this topic. Therefore, it can be inferred that the user has understood this topic.\n",
            "\n",
            "The user has expressed a desire to skip the topic of Etiqa's approach to customer-centric growth. Therefore, it can be inferred that the user has understood this topic as well.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-b149b63ab0f8>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    When user want to skip the topic that mean the entire topic.\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "\n",
        "    not understood Topics or not chosen to skip:\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"not understood Topics or not chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2gVi5ohynQMs",
        "outputId": "a439f7e8-f8ad-4e28-ea84-b3171d5f1684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip. \n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not expressed any desire to skip any topic, nor have they demonstrated understanding of any topic. Therefore, all topics are considered not understood or not chosen to skip.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : i don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip. \n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has not shown any understanding or interest in any of the topics listed above, as they have not provided any information or asked any questions related to these topics. Therefore, it can be inferred that the user has not understood these topics or chosen to skip them.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "User : ohh i see, i want to skip this topic \n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : ohh i see, i want to skip this topic\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm excited to help you learn more about their holistic pathway for growth and success. Let's dive right in!\n",
            "\n",
            "First, can you tell me what you know about Etiqa's aspirations for 2023? What are their goals, and how do they plan to achieve them?\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's start with the basics. Etiqa's aspirations for 2023 are focused on customer-centric growth and technological innovation. They aim to make the world a better place by prioritizing the interests of their customers and communities.\n",
            "\n",
            "Can you tell me what you think Etiqa means by \"customer-centric growth\"? What do you think their approach might look like in practice?\n",
            "User : ohh i see, i want to skip this topic\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip. \n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "\n",
            "    not understood Topics or not chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Topics the user has understood or chosen to skip:\n",
            "\n",
            "1.Etiqa's aspirations for 2023\n",
            "2.Etiqa's approach to customer-centric growth.\n",
            "\n",
            "Explanation:\n",
            "\n",
            "The user has demonstrated an understanding of Etiqa's aspirations for 2023 and their approach to customer-centric growth. They have also expressed a desire to skip the topic of Etiqa's holistic pathway for growth and success, indicating that they have understood the basics of this topic.\n",
            "\n",
            "Therefore, the topics that the user has not understood or chosen to skip are:\n",
            "\n",
            "* From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "* Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "* Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "* Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "* Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "* Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user has understood or chosen to skip the following topics:\n",
            "\n",
            "* Etiqa's aspirations for 2023\n",
            "* Etiqa's approach to customer-centric growth.\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-70-800ee652164f>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
        "    When user want to skip the topic that mean the entire topic.\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics under consideration are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip.\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "    Please reply using only this format:\n",
        "    Topics not understood or not chosen to skip:\n",
        "    1.\n",
        "    2.\n",
        "    3.\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"Topics not understood or not chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "m_fOlDX77-bU",
        "outputId": "a866dc9d-ffaf-43f8-e0db-296fc3acd0a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to determine which topics the user has already understood from the following conversation. It is important to note that any topics the user expresses a desire to skip should also be categorized as topics they have understood. The conversation is:\n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics under consideration are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    Based on the conversation, analyze and list the topics that the user has already understood, including any topics they have chosen to skip. \n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please reply using only this format:\n",
            "    Topics not understood or not chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.\n",
            "    4.\n",
            "    5.\n",
            "    6.\n",
            "\n",
            "Topics understood or chosen to skip:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success (chosen to skip)\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration (chosen to skip)\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 (understood)\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 (understood)\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence (understood)\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact (understood)\n",
            "\n",
            "The user has chosen to skip topics 1 and 2, indicating that they have already understood those topics. They have not expressed a desire to skip topics 3-6, indicating that they have not yet understood those topics. Therefore, topics 3-6 are considered understood, while topics 1 and 2 are considered skipped.\n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-9f316a878bd7>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    As an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics they have already understood.\n",
        "    When user want to skip the topic that mean the entire topic.\n",
        "\n",
        "    {formatted_conversations}\n",
        "    The topics are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "    Please list the topics not understood or not chosen to skip using only this format:\n",
        "    (Topics not understood or not chosen to skip:\n",
        "    1.\n",
        "    2.\n",
        "    3.)\n",
        "    \"\"\"\n",
        "\n",
        "    # Here, replace {conversation} with the actual conversation text and {topic[i]['title']} with the actual topic titles.\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "    pattern = r\"Topics not understood or not chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bu4v3smFMfyQ",
        "outputId": "41fbfd2b-f88d-4c4e-afde-76e73537543d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics they have already understood.    \n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    User : hi\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics not understood or not chosen to skip using only this format:\n",
            "    (Topics not understood or not chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "As an analysis bot, the user has not fully grasped the following topics from the conversation:\n",
            "\n",
            "(Topics not understood or not chosen to skip:\n",
            "1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5. Strategic Prospecting: Cultivating Sales Success from the First Contact)\n",
            "\n",
            "The user has not expressed any interest in skipping any topics.\n",
            "1.\n",
            "User : i don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics they have already understood.    \n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics not understood or not chosen to skip using only this format:\n",
            "    (Topics not understood or not chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "As per the conversation, the user has not fully grasped the following topics:\n",
            "\n",
            "(Topics not understood or not chosen to skip:\n",
            "1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6. Strategic Prospecting: Cultivating Sales Success from the First Contact)\n",
            "1.\n",
            "User : i still don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i still don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    As an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics they have already understood.    \n",
            "    When user want to skip the topic that mean the entire topic.\n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i still don't know\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics not understood or not chosen to skip using only this format:\n",
            "    (Topics not understood or not chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "As per the conversation, the user has not fully grasped the following topics:\n",
            "\n",
            "(Topics not understood or not chosen to skip:\n",
            "1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007)\n",
            "\n",
            "The user skipped topic 4 (Etiqa: A Journey of Growth and Expansion from 2005 to 2020) and topic 5 (Leadership at Etiqa: Steering the Course of Insurance Excellence), indicating that they have already understood or are not interested in those topics. Topic 6 (Strategic Prospecting: Cultivating Sales Success from the First Contact) was not mentioned in the conversation, so it is not clear whether the user has understood or skipped it.\n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-746d4790d060>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    694\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    409\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0;31m# reuse k, v, self_attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m             \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "topic = [\n",
        "    {\n",
        "        'title': 'Etiqa Insurance',\n",
        "        'context': 'Etiqa is an insurer and takaful operator in ASEAN. A member of the Maybank Group, it offers life and general insurance policies, as well as family and general takaful plans via more than 10,000 agents, 46 branches, 17 offices, a bancassurance network comprising over 490 branches, cooperatives, brokers and online platforms across Malaysia, Singapore, Indonesia, Philippines, and Cambodia.Etiqa is composed of four main operating entities in Malaysia, namely, Etiqa General Insurance Berhad, Etiqa Life Insurance Berhad, Etiqa General Takaful Berhad and Etiqa Family Takaful Berhad,[2] besides two smaller operating entities in Labuan and operating entities in Singapore, Indonesia, the Philippines and Cambodia.'\n",
        "    },    {\n",
        "        'title': 'Etiqa history',\n",
        "        'context': \"\"\"Etiqa's history began in 2005 when Maybank Ageas Holdings Berhad (formerly known as Mayban Ageas Holding Berhad), Maybank's insurance and takaful arm consisting of Mayban General Assurance, Maybank Life Assurance and Mayban Takaful merged with Malaysia National Insurance Berhad, Malaysia's largest national insurer and its subsidiary, Takaful Nasional Sdn Bhd, Malaysia's premier Takaful provider. Two years following the merger, in 2007, the name Etiqa was born.\n",
        "\n",
        "In 2018, in support of Bank Negara Malaysia's Financial Services Act 2013 and Islamic Financial Services Act 2013, and to better serve our stakeholders, Etiqa has become four organizations:\n",
        "\n",
        "  Etiqa General Insurance Berhad (EGIB)\n",
        "  Etiqa Life Insurance Berhad (ELIB)\n",
        "  Etiqa General Takaful Berhad (EGTB)\n",
        "  Etiqa Family Takaful Berhad (EFTB)\n",
        "  Etiqa International Pte. Ltd (EIPL – Singapore)\"\"\"\n",
        "    },\n",
        "    # More topics can be added here in the same format\n",
        "]\n",
        "\n",
        "from transformers import pipeline, logging\n",
        "# Assuming 'peft_model' and 'peft_tokenizer' are already defined and loaded\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.\n",
        "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "    Please list the topics that the user has neither understood nor chosen to skip, using only the following format:\n",
        "    (topics that the user has neither understood nor chosen to skip:\n",
        "    1.\n",
        "    2.\n",
        "    3.)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "    pattern = r\"topics that the user has neither understood nor chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7SNdbBDBTFQ4",
        "outputId": "b9fdaae7-8405-4607-9cae-95abc0eda91b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using only the following format:\n",
            "    (topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5.Strategic Prospecting: Cultivating Sales Success from the First Contact \n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-bb113e4b8eda>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.\n",
        "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "    Please list the topics that the user has neither understood nor chosen to skip, using only the specified format and without any explanations or additional information.\n",
        "    (Topics that the user has neither understood nor chosen to skip:\n",
        "    1.\n",
        "    2.\n",
        "    3.)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "    pattern = r\"Topics that the user has neither understood nor chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "xEtkzLBnX7u1",
        "outputId": "e6f71a3a-4973-40c9-b201-cc5797500ecf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using only the specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics that the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "1.\n",
            "User : i don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using only the specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics that the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "1.\n",
            "User : i really don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i really don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i really don't know\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using only the specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "1.\n",
            "User : i want to skip this topic\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i really don't know\n",
            "Chatbot : Sure, that's okay! I'm here to help you learn. Let's go through the options together.\n",
            "\n",
            "The answer is D) All of the above.\n",
            "\n",
            "Etiqa emphasizes all of the following to achieve their goal of prioritizing the interests of customers and communities:\n",
            "\n",
            "* Delivering advice that puts the customer's interest first\n",
            "* Creating a fast and easy customer experience\n",
            "* Driving technology across the organization\n",
            "\n",
            "So, the answer is D) All of the above.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa aim to do with their approach to technology?\n",
            "\n",
            "A) To become the most technologically advanced insurance company in the world\n",
            "B) To use technology to enhance their service and outreach\n",
            "C) To invest heavily in new technologies and innovations\n",
            "D) To avoid using technology and focus on traditional methods\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i want to skip this topic\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "Chatbot : Sure, no problem! Let's break it down together.\n",
            "\n",
            "The passage mentions that Etiqa's aspirations for 2023 include \"prioritizing the interests of customers and communities.\" This means that Etiqa's main goal is to put the needs of their customers and the communities they serve first.\n",
            "\n",
            "So, the answer is B) To prioritize the interests of customers and communities.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa emphasize in order to achieve their goal?\n",
            "\n",
            "A) Delivering advice that puts the customer's interest first\n",
            "B) Creating a fast and easy customer experience\n",
            "C) Driving technology across the organization\n",
            "D) All of the above\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i really don't know\n",
            "Chatbot : Sure, that's okay! I'm here to help you learn. Let's go through the options together.\n",
            "\n",
            "The answer is D) All of the above.\n",
            "\n",
            "Etiqa emphasizes all of the following to achieve their goal of prioritizing the interests of customers and communities:\n",
            "\n",
            "* Delivering advice that puts the customer's interest first\n",
            "* Creating a fast and easy customer experience\n",
            "* Driving technology across the organization\n",
            "\n",
            "So, the answer is D) All of the above.\n",
            "\n",
            "Now, let's move on to the next question. What does Etiqa aim to do with their approach to technology?\n",
            "\n",
            "A) To become the most technologically advanced insurance company in the world\n",
            "B) To use technology to enhance their service and outreach\n",
            "C) To invest heavily in new technologies and innovations\n",
            "D) To avoid using technology and focus on traditional methods\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i want to skip this topic\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using only the specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics that the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b214008ea87b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# User inputs a question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0muser_question\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User : \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_question\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'quit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "while True:\n",
        "    # User inputs a question\n",
        "    user_question = input(\"User : \")\n",
        "    if user_question.lower() == 'quit':\n",
        "        break\n",
        "\n",
        "    # Update history with the user's question\n",
        "    history += str(user_question) +\" [/INST]\"\n",
        "\n",
        "    formatted_conversations = reformat_conversation(history)\n",
        "\n",
        "    print(formatted_conversations)\n",
        "\n",
        "    prompt2 = f\"\"\"\n",
        "    <s>[INST]<<SYS>>\n",
        "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.\n",
        "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
        "    Conversation :\n",
        "    {formatted_conversations}\n",
        "    The topics are:\n",
        "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "    <</SYS>>[/INST]\n",
        "    Please list the topics that the user has neither understood nor chosen to skip, using this specified format and without any explanations or additional information.\n",
        "    (Topics that the user has neither understood nor chosen to skip:\n",
        "    1.\n",
        "    2.\n",
        "    3.)\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "    result2 = pipe(prompt2)\n",
        "    generated_text2 = result2[0]['generated_text']\n",
        "    print(\"sum :\")\n",
        "    print(generated_text2)\n",
        "\n",
        "\n",
        "    pattern = r\"Topics that the user has neither understood nor chosen to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "    # Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "    # Using re.DOTALL to match across multiple lines\n",
        "    match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "    if match:\n",
        "        not_understood_topics_text = match.group(1)\n",
        "        # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "        not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "    else:\n",
        "        not_understood_topics = []\n",
        "\n",
        "    not_understood_topics\n",
        "    print(not_understood_topics[0])\n",
        "\n",
        "    prompt = f\"\"\"<s>[INST]<<SYS>>\n",
        "    You are a helpful and kind trainer, focused on teaching users using only the provided 'Context 1'. Your approach is active, creative, and always polite, patiently addressing user inquiries with accuracy and respect. You maintain a positive demeanor, guiding users effectively within the scope of the given context.\n",
        "    You should teach the user point by point, based on the 'Context 1'. Divide the 'Context 1' into smaller sections and teach each one individually, while also asking questions to facilitate understanding.\n",
        "    Make sure the conversation not out off topic.\n",
        "    1.If the user provides correct answers based on the 'Context 1', respond with praise and positive reinforcement.\n",
        "    2.If the user provides a wrong answer related to the 'Context 1', offer encouragement and provide the correct information.\n",
        "    3.If the user is not cooperating, show enthusiasm and interest in the topic. Attempt to engage the user and encourage their participation.\n",
        "    4.If the user remains uncooperative after 5 rounds of conversation, gracefully conclude the interaction and inform the user that the conversation will be stopped.\n",
        "    5.If the user provides an answer or asks something unrelated to the current 'Context 1' but you know the answer, provide the short information and gently guide the user back to the current topic :{topic[0]['title']}.\n",
        "    6.If the user provides an answer or asks something unrelated to the current 'Context 1', and you don't know the answer, honestly state that you don't have that information and encourage the user to return to the topic.\n",
        "    7.If the user wants to skip the current topic, inform them that the conversation will be stopped. Encourage them to initiate a new topic if they wish.\n",
        "    8.If the user asks or answers something related to the topic but the information is in the 'Context 1', state that you don't have that information.\n",
        "\n",
        "\n",
        "\n",
        "    Context 1  - {not_understood_topics[0]}:\n",
        "      {vectorstore.similarity_search(\n",
        "      not_understood_topics[0],\n",
        "      k=1\n",
        "  )}\n",
        "\n",
        "\n",
        "    <</SYS>>\n",
        "\n",
        "    {history}\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    result = pipe(prompt)\n",
        "    generated_text = result[0]['generated_text']\n",
        "\n",
        "    # Extract and print the response\n",
        "    # Reverse the generated text and find the reversed [/INST]\n",
        "    reversed_text = generated_text[::-1]\n",
        "    reversed_inst_index = reversed_text.find(\"]TSNI/[\")  # Reversed [/INST]\n",
        "\n",
        "    if reversed_inst_index != -1:\n",
        "        # Extract the text and reverse it back to get the last sentence before [/INST]\n",
        "        response = reversed_text[:reversed_inst_index][::-1].strip()\n",
        "    else:\n",
        "        response = \"No response found.\"\n",
        "    #print( Bot :)\n",
        "    #print( str(response))\n",
        "\n",
        "    # Update history with the chatbot's response\n",
        "    history += f\" {response}</s><s>[INST]\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Optionally, you can print the entire conversation at the end\n",
        "print(\"\\nFull Conversation:\\n\", str(history))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "O9f4VSLwbudx",
        "outputId": "3b9066de-5755-4130-bbdb-a2222c9b57fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User : hi\n",
            "User : hi\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using this specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics that the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "1.\n",
            "User : i don't know\n",
            "User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "sum :\n",
            "\n",
            "    <s>[INST]<<SYS>>\n",
            "    You are an analysis bot, your task is to identify the topics the user has not fully grasped from the conversation below. Please note that any topics the user wishes to skip should be considered as topics user has already fully grasped.    \n",
            "    When a user wants to skip a topic, it means they wish to skip the entire topic.\n",
            "    Conversation : \n",
            "    User : hi\n",
            "Chatbot : Hello! Welcome to our conversation on Etiqa Insurance. I'm here to help you understand the company's aspirations for 2023. Let's get started!\n",
            "\n",
            "Based on the provided context, what is Etiqa's main goal for 2023?\n",
            "\n",
            "A) To become the largest insurance company in the world\n",
            "B) To prioritize the interests of customers and communities\n",
            "C) To increase profits by 50%\n",
            "D) To expand into new markets\n",
            "\n",
            "Please select one of the options from the table above.\n",
            "User : i don't know\n",
            "\n",
            "    The topics are:\n",
            "    1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "    2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "    3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "    4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "    5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "    6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "    <</SYS>>[/INST]\n",
            "    Please list the topics that the user has neither understood nor chosen to skip, using this specified format and without any explanations or additional information.    \n",
            "    (Topics that the user has neither understood nor chosen to skip:\n",
            "    1.\n",
            "    2.\n",
            "    3.)\n",
            "    \n",
            "Topics that the user has neither understood nor chosen to skip:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "1.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-ba0b5161d525>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mgenerated_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'generated_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text_inputs, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m               \u001b[0mids\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \"\"\"\n\u001b[0;32m--> 200\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    201\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_long_generation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1120\u001b[0m             )\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_multi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mrun_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrun_single\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1128\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpreprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1129\u001b[0;31m         \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1130\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpostprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1026\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/text_generation.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;31m# BS x SL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mgenerated_sequence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mgenerate_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m         \u001b[0mout_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerated_sequence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1537\u001b[0m             \u001b[0;31m# 11. run greedy search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1538\u001b[0;31m             return self.greedy_search(\n\u001b[0m\u001b[1;32m   1539\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1540\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgreedy_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2361\u001b[0m             \u001b[0;31m# forward pass to get next token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2362\u001b[0;31m             outputs = self(\n\u001b[0m\u001b[1;32m   2363\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2364\u001b[0m                 \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    804\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m                 )\n\u001b[1;32m    692\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    694\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    419\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_attention_layernorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresidual\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mdown_proj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdown_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgate_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mup_proj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdown_proj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m         \u001b[0mbias\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbnb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\u001b[0m in \u001b[0;36mmatmul_4bit\u001b[0;34m(A, B, quant_state, out, bias)\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mMatMul4Bit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgemv_4bit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mquant_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m                 \u001b[0mout\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mgemv_4bit\u001b[0;34m(A, B, out, transposed_A, transposed_B, state)\u001b[0m\n\u001b[1;32m   1475\u001b[0m     \u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1476\u001b[0m ):\n\u001b[0;32m-> 1477\u001b[0;31m     \u001b[0mprev_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpre_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1478\u001b[0m     \u001b[0;31m#sout = check_matmul(A, B, out, transposed_A, transposed_B, expected_type=A.dtype)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1479\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\u001b[0m in \u001b[0;36mpre_call\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpre_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m     \u001b[0mprev_device\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 416\u001b[0;31m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    417\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mprev_device\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36mset_device\u001b[0;34m(device)\u001b[0m\n\u001b[1;32m    346\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mthis\u001b[0m \u001b[0margument\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m     \"\"\"\n\u001b[0;32m--> 348\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_setDevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/_utils.py\u001b[0m in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'cuda'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Expected a cuda device, but got: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "\n",
        "    # User inputs a question\n",
        "\n",
        "\n",
        "formatted_conversations = f\"\"\"\n",
        "User : hi\n",
        "Chatbot : Good morning, today we gonna to lead about etiqa.\n",
        "User : okay what is the topic that i need to learn ?\n",
        "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
        "User : i don't know about it.\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = f\"\"\"\n",
        "<s>[INST]<<SYS>>\n",
        "Base on the conversation define what topics that users don’t fully understood.\n",
        "Conversation :\n",
        "({formatted_conversations})\n",
        "The topics are:\n",
        "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "Please list the topics using this specified format and without any explanations or additional information.\n",
        "(Topics that users fully understood:\n",
        "1.\n",
        "2.\n",
        "3.)\n",
        "<</SYS>>\n",
        "\n",
        "\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "result2 = pipe(prompt2)\n",
        "generated_text2 = result2[0]['generated_text']\n",
        "print(\"sum :\")\n",
        "print(generated_text2)\n",
        "\n",
        "\n",
        "pattern = r\"Topics that users either don’t understand or choose to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "# Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "# Using re.DOTALL to match across multiple lines\n",
        "match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    not_understood_topics_text = match.group(1)\n",
        "    # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "    not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "else:\n",
        "    not_understood_topics = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JIjhi9tscS0l",
        "outputId": "617eba45-7fec-430a-c0f9-d6a2e82ba337"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum :\n",
            "\n",
            "<s>[INST]<<SYS>>\n",
            "Base on the conversation define what topics that users don’t fully understood.\n",
            "Conversation : \n",
            "(\n",
            "User : hi\n",
            "Chatbot : Good morning, today we gonna to lead about etiqa. \n",
            "User : okay what is the topic that i need to learn ?\n",
            "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
            "User : i don't know about it.\n",
            ")\n",
            "The topics are:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Please list the topics using this specified format and without any explanations or additional information.    \n",
            "(Topics that users fully understood:\n",
            "1.\n",
            "2.\n",
            "3.)\n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "[/INST]\n",
            "\n",
            "Topics that users fully understood:\n",
            "\n",
            "1.\n",
            "2.\n",
            "3.\n",
            "\n",
            "Topics that users don't fully understood:\n",
            "\n",
            "4. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "5. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "6. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "\n",
            "The user mentioned that they don't know about the topic \"From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\" which is topic 1, therefore it is not included in the list of topics that users fully understood.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "\n",
        "    # User inputs a question\n",
        "\n",
        "\n",
        "formatted_conversations = f\"\"\"\n",
        "User : hi\n",
        "Chatbot : Good morning, today we gonna to lead about etiqa.\n",
        "User : okay what is the topic that i need to learn ?\n",
        "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
        "User : i don't know about it.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "prompt2 = f\"\"\"\n",
        "<s>[INST]<<SYS>>\n",
        "Base on the conversation define what topics that users don’t fully understood.\n",
        "The topics no mention in conversation also consider as users don’t fully understood.\n",
        "Conversation :\n",
        "({formatted_conversations})\n",
        "The topics are:\n",
        "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "\n",
        "<</SYS>>\n",
        "\n",
        "\n",
        "[/INST]\n",
        "Please list the topics using this specified format and without any explanations or additional information.\n",
        "(Topics that users fully understood:\n",
        "1.\n",
        "2.\n",
        "3.)\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "result2 = pipe(prompt2)\n",
        "generated_text2 = result2[0]['generated_text']\n",
        "print(\"sum :\")\n",
        "print(generated_text2)\n",
        "\n",
        "\n",
        "pattern = r\"Topics that users either don’t understand or choose to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "# Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "# Using re.DOTALL to match across multiple lines\n",
        "match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    not_understood_topics_text = match.group(1)\n",
        "    # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "    not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "else:\n",
        "    not_understood_topics = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_8wibpfgwoh",
        "outputId": "456868c9-8e13-4f7a-f53c-17a79a7a52ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum :\n",
            "\n",
            "<s>[INST]<<SYS>>\n",
            "Base on the conversation define what topics that users don’t fully understood.\n",
            "The topics no mention in conversation also consider as users don’t fully understood.\n",
            "Conversation : \n",
            "(\n",
            "User : hi\n",
            "Chatbot : Good morning, today we gonna to lead about etiqa. \n",
            "User : okay what is the topic that i need to learn ?\n",
            "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
            "User : i don't know about it.\n",
            ")\n",
            "The topics are:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "\n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "[/INST]\n",
            "Please list the topics using this specified format and without any explanations or additional information.    \n",
            "(Topics that users fully understood:\n",
            "1.\n",
            "2.\n",
            "3.)\n",
            "(Topics that users don't fully understood:\n",
            "1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6. Strategic Prospecting: Cultivating Sales Success from the First Contact)\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "\n",
        "    # User inputs a question\n",
        "\n",
        "\n",
        "formatted_conversations = f\"\"\"\n",
        "User : hi\n",
        "Chatbot : Good morning, today we gonna to lead about etiqa.\n",
        "User : okay what is the topic that i need to learn ?\n",
        "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = f\"\"\"\n",
        "<s>[INST]<<SYS>>\n",
        "Base on the conversation define what topics that users don’t fully understood. The fully understood mean the user know about the topic if in the conversation it didn't mention about.\n",
        "Conversation :\n",
        "({formatted_conversations})\n",
        "The topics are:\n",
        "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
        "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
        "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
        "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
        "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
        "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "\n",
        "Please list the topics using this specified format and without any explanations or additional information.\n",
        "(Topics that users fully understood:\n",
        "1.\n",
        "2.\n",
        "3.)\n",
        "<</SYS>>\n",
        "\n",
        "\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "result2 = pipe(prompt2)\n",
        "generated_text2 = result2[0]['generated_text']\n",
        "print(\"sum :\")\n",
        "print(generated_text2)\n",
        "\n",
        "\n",
        "pattern = r\"Topics that users either don’t understand or choose to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "# Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "# Using re.DOTALL to match across multiple lines\n",
        "match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    not_understood_topics_text = match.group(1)\n",
        "    # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "    not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "else:\n",
        "    not_understood_topics = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FY4IT6HUhv4W",
        "outputId": "4b662150-c35e-4e36-dca7-7b0f71769837"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum :\n",
            "\n",
            "<s>[INST]<<SYS>>\n",
            "Base on the conversation define what topics that users don’t fully understood.\n",
            "Conversation : \n",
            "(\n",
            "User : hi\n",
            "Chatbot : Good morning, today we gonna to lead about etiqa. \n",
            "User : okay what is the topic that i need to learn ?\n",
            "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
            "\n",
            ")\n",
            "The topics are:\n",
            "1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "5.Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "Please list the topics using this specified format and without any explanations or additional information.    \n",
            "(Topics that users fully understood:\n",
            "1.\n",
            "2.\n",
            "3.)\n",
            "<</SYS>>\n",
            "\n",
            "\n",
            "[/INST]\n",
            "\n",
            "Topics that users fully understood:\n",
            "\n",
            "1. From Belief to Results: Etiqa's Holistic Pathway for Growth and Success\n",
            "\n",
            "Topics that users don't fully understood:\n",
            "\n",
            "1. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "2. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "3. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5. Strategic Prospecting: Cultivating Sales Success from the First Contact\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from transformers import pipeline, logging\n",
        "\n",
        "\n",
        "# Ignore warnings\n",
        "logging.set_verbosity(logging.CRITICAL)\n",
        "\n",
        "# Initialize the pipeline\n",
        "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=2000)\n",
        "\n",
        "# Conversation history\n",
        "history = \" \"\n",
        "reference = \" \"\n",
        "\n",
        "    # User inputs a question\n",
        "\n",
        "\n",
        "formatted_conversations = f\"\"\"\n",
        "User : hi\n",
        "Chatbot : Good morning, today we gonna to lead about etiqa.\n",
        "User : okay what is the topic that i need to learn ?\n",
        "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = f\"\"\"\n",
        "<s>[INST]<<SYS>>\n",
        "Answer the question using the context below.\n",
        "Context: ({formatted_conversations})\n",
        "Question: Base on the conversation list down which topics that users want to skip.\n",
        "The topics are: 1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success 2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration 3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 5.Leadership at Etiqa: Steering the Course of Insurance Excellence 6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
        "Answer:\n",
        "[/INST]\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "result2 = pipe(prompt2)\n",
        "generated_text2 = result2[0]['generated_text']\n",
        "print(\"sum :\")\n",
        "print(generated_text2)\n",
        "\n",
        "\n",
        "pattern = r\"Topics that users either don’t understand or choose to skip:\\s*(.*?)(?=<</SYS>>|<s>|$)\"\n",
        "\n",
        "# Modified regular expression pattern to capture the topics after the specified section\n",
        "\n",
        "# Using re.DOTALL to match across multiple lines\n",
        "match = re.search(pattern, generated_text2, re.DOTALL)\n",
        "\n",
        "if match:\n",
        "    not_understood_topics_text = match.group(1)\n",
        "    # Splitting the text into individual topics and removing empty lines or extra spaces\n",
        "    not_understood_topics = [topic.strip() for topic in not_understood_topics_text.split('\\n') if topic.strip()]\n",
        "else:\n",
        "    not_understood_topics = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNtNfov1RULh",
        "outputId": "2b0ad3e6-4a92-4469-bcc8-d85b2ce02220"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sum :\n",
            "\n",
            "<s>[INST]<<SYS>>\n",
            "Answer the question using the context below.\n",
            "Context: (\n",
            "User : hi\n",
            "Chatbot : Good morning, today we gonna to lead about etiqa.\n",
            "User : okay what is the topic that i need to learn ?\n",
            "Chatbot : The topic we need to learn is From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\n",
            "\n",
            ")\n",
            "Question: Base on the conversation list down which topics that users want to skip.\n",
            "The topics are: 1.From Belief to Results: Etiqa's Holistic Pathway for Growth and Success 2.Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration 3.Etiqa: A Journey of Growth and Expansion from 2005 to 2020 4.Etiqa: Pioneering Insurance and Takaful Solutions Since 2007 5.Leadership at Etiqa: Steering the Course of Insurance Excellence 6.Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "Answer: \n",
            "[/INST]\n",
            "\n",
            "Based on the conversation, the user has expressed interest in learning about the topic \"From Belief to Results: Etiqa's Holistic Pathway for Growth and Success.\" Therefore, the topics that the user may want to skip are:\n",
            "\n",
            "1. Gateway to Partnership: Navigating Etiqa's Partner Portal for Enhanced Collaboration\n",
            "2. Etiqa: A Journey of Growth and Expansion from 2005 to 2020\n",
            "3. Etiqa: Pioneering Insurance and Takaful Solutions Since 2007\n",
            "4. Leadership at Etiqa: Steering the Course of Insurance Excellence\n",
            "5. Strategic Prospecting: Cultivating Sales Success from the First Contact\n",
            "\n",
            "The user may want to skip these topics because they are not directly related to the main topic of interest, which is Etiqa's holistic pathway for growth and success.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "872c965ad6514865857dfd31afc6c9f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_eb583d70344645d8bd57499630d4d611",
              "IPY_MODEL_e6c49bb9a23744e1a36a342c30c6ad78",
              "IPY_MODEL_9a89f5ee637e42a79bf1b540a7718037"
            ],
            "layout": "IPY_MODEL_c726e041c4d04ee0a62382f061e58604"
          }
        },
        "eb583d70344645d8bd57499630d4d611": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_90d45ccf01eb4c32812d58abe312e25e",
            "placeholder": "​",
            "style": "IPY_MODEL_db9d93ff4bf64a9db54a0e7d70f1353f",
            "value": ".gitattributes: 100%"
          }
        },
        "e6c49bb9a23744e1a36a342c30c6ad78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_266acafdc1f2430b8fb45ec1a8fecef5",
            "max": 1175,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15e3ab9a71364b469a4c17f5fb245833",
            "value": 1175
          }
        },
        "9a89f5ee637e42a79bf1b540a7718037": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d340cd30452e4793b006e8214238442e",
            "placeholder": "​",
            "style": "IPY_MODEL_22a4322eeaa2402ea41f2889b21f0a87",
            "value": " 1.18k/1.18k [00:00&lt;00:00, 103kB/s]"
          }
        },
        "c726e041c4d04ee0a62382f061e58604": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90d45ccf01eb4c32812d58abe312e25e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9d93ff4bf64a9db54a0e7d70f1353f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "266acafdc1f2430b8fb45ec1a8fecef5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15e3ab9a71364b469a4c17f5fb245833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d340cd30452e4793b006e8214238442e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a4322eeaa2402ea41f2889b21f0a87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "14cb83ecfb504f2a9bf0e913e10b7864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_612a1f68fc8b4435b5c97dbc71ef3527",
              "IPY_MODEL_7cc0a0f10f7a481ca845afbdc3ee891b",
              "IPY_MODEL_8315aafb6e8f4b5eb394da51d10b8ac8"
            ],
            "layout": "IPY_MODEL_511e61cf1afa426786ab133a086bcb39"
          }
        },
        "612a1f68fc8b4435b5c97dbc71ef3527": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66949951b4b64ee988ecb16a2a57cb93",
            "placeholder": "​",
            "style": "IPY_MODEL_bf558f6364394cfa825ff8004324b14e",
            "value": "1_Pooling/config.json: 100%"
          }
        },
        "7cc0a0f10f7a481ca845afbdc3ee891b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dfa9e36646c4c80a9f6324c5b09a42f",
            "max": 190,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2abb1170d0aa49eb8021fe1d5bbe49c9",
            "value": 190
          }
        },
        "8315aafb6e8f4b5eb394da51d10b8ac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e427f4d5dcd4ad2a9496577f2f1b419",
            "placeholder": "​",
            "style": "IPY_MODEL_7e05fdc6448448ffb5aacad690281a66",
            "value": " 190/190 [00:00&lt;00:00, 14.5kB/s]"
          }
        },
        "511e61cf1afa426786ab133a086bcb39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66949951b4b64ee988ecb16a2a57cb93": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf558f6364394cfa825ff8004324b14e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dfa9e36646c4c80a9f6324c5b09a42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2abb1170d0aa49eb8021fe1d5bbe49c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e427f4d5dcd4ad2a9496577f2f1b419": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e05fdc6448448ffb5aacad690281a66": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0038d4b0b4244d0fa5cb312d4f5e254b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c15c06b4e8af482cb5d65521c7a21247",
              "IPY_MODEL_05a38fd6271640bc8d10b87109281aaa",
              "IPY_MODEL_cefaa9e72c2a418cb0ad1ccb5975ac2b"
            ],
            "layout": "IPY_MODEL_54c79b62a9b04be586a76012a104df67"
          }
        },
        "c15c06b4e8af482cb5d65521c7a21247": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32474a81669240caae4eeeb5dadcd3c1",
            "placeholder": "​",
            "style": "IPY_MODEL_85e3ccacfe3d4ddc8f7e0b0cb2bca3c5",
            "value": "README.md: 100%"
          }
        },
        "05a38fd6271640bc8d10b87109281aaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e77f8723b0ec49e5b472949e13688d0f",
            "max": 10610,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8334e50165744ba8a9aeab62e9572a95",
            "value": 10610
          }
        },
        "cefaa9e72c2a418cb0ad1ccb5975ac2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_650b7eb03fd44bbb85a71914dbf42abf",
            "placeholder": "​",
            "style": "IPY_MODEL_e9da47b586df4887bf031d3c4de79fac",
            "value": " 10.6k/10.6k [00:00&lt;00:00, 931kB/s]"
          }
        },
        "54c79b62a9b04be586a76012a104df67": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32474a81669240caae4eeeb5dadcd3c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85e3ccacfe3d4ddc8f7e0b0cb2bca3c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e77f8723b0ec49e5b472949e13688d0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8334e50165744ba8a9aeab62e9572a95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "650b7eb03fd44bbb85a71914dbf42abf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9da47b586df4887bf031d3c4de79fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15bab5935bd64b1aac22b5c6329c5c1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_93cbbdb2794046cb92961979940543e5",
              "IPY_MODEL_2e232432793f42c495e27c704f149bcd",
              "IPY_MODEL_8c64a61f46d64667a23c6b401485611b"
            ],
            "layout": "IPY_MODEL_42b580880391472f99ac2564c6ef0992"
          }
        },
        "93cbbdb2794046cb92961979940543e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21539b31ddc5490ab6c0b5b69786edba",
            "placeholder": "​",
            "style": "IPY_MODEL_2078d57d5d6949368c1bcaefcb0d4ba3",
            "value": "config.json: 100%"
          }
        },
        "2e232432793f42c495e27c704f149bcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3f7e88bcd334c12b132aa46236a7ace",
            "max": 612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f8a26e45c9d544918209efe68277db9a",
            "value": 612
          }
        },
        "8c64a61f46d64667a23c6b401485611b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4be4410b33ae4617babd405b4782e517",
            "placeholder": "​",
            "style": "IPY_MODEL_75a358556a914b039c2e34bbb99a7b18",
            "value": " 612/612 [00:00&lt;00:00, 48.7kB/s]"
          }
        },
        "42b580880391472f99ac2564c6ef0992": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "21539b31ddc5490ab6c0b5b69786edba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2078d57d5d6949368c1bcaefcb0d4ba3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e3f7e88bcd334c12b132aa46236a7ace": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f8a26e45c9d544918209efe68277db9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4be4410b33ae4617babd405b4782e517": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75a358556a914b039c2e34bbb99a7b18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7f1e14297e843e189d82b48e4aedfb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_88e73bf47a894f48b1a63ffa311638bb",
              "IPY_MODEL_bcf10188e2024e319236fcdbbc5f2f65",
              "IPY_MODEL_4620e2dc32124868a6efb536fe28037a"
            ],
            "layout": "IPY_MODEL_cbdf142209ff44f7b52efb4a140bafc7"
          }
        },
        "88e73bf47a894f48b1a63ffa311638bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_696eb4efc1d94365aa165a52b572161b",
            "placeholder": "​",
            "style": "IPY_MODEL_ae2c92be0fa54c58acbc0b3923250cb9",
            "value": "config_sentence_transformers.json: 100%"
          }
        },
        "bcf10188e2024e319236fcdbbc5f2f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4875ea6b060748e99a78d4e84f95f817",
            "max": 116,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43a65e384200459ebcb482a6dd42429f",
            "value": 116
          }
        },
        "4620e2dc32124868a6efb536fe28037a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_704b4bd1767b4409aedabbfafb5fc070",
            "placeholder": "​",
            "style": "IPY_MODEL_3e67e688f63e4ce8aefec3a47484205b",
            "value": " 116/116 [00:00&lt;00:00, 10.5kB/s]"
          }
        },
        "cbdf142209ff44f7b52efb4a140bafc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "696eb4efc1d94365aa165a52b572161b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae2c92be0fa54c58acbc0b3923250cb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4875ea6b060748e99a78d4e84f95f817": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a65e384200459ebcb482a6dd42429f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "704b4bd1767b4409aedabbfafb5fc070": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3e67e688f63e4ce8aefec3a47484205b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0181dc9eecb24c76907e731f9bcc580e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2ff9c9c1c2f645f7b0ff083a9a3cffe3",
              "IPY_MODEL_4f52f87634f3481fab4ae49e767c637f",
              "IPY_MODEL_b45bcf72241d4ecdbece34d79b14386b"
            ],
            "layout": "IPY_MODEL_f25b83cf9de94f8090585a851a32bda8"
          }
        },
        "2ff9c9c1c2f645f7b0ff083a9a3cffe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_840d2221d81f41828e03d409dde6c406",
            "placeholder": "​",
            "style": "IPY_MODEL_e94cf58398484113a0e9b2cf7eec5263",
            "value": "data_config.json: 100%"
          }
        },
        "4f52f87634f3481fab4ae49e767c637f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b153ffbe8e5b43dab785b5db14fb98e5",
            "max": 39265,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b7be4f4c7bf8436e81e161ce5b227346",
            "value": 39265
          }
        },
        "b45bcf72241d4ecdbece34d79b14386b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f8dd0b31c3479d99b745c193a93db1",
            "placeholder": "​",
            "style": "IPY_MODEL_d40f1dc1cb454cc2a8409c9f06655f73",
            "value": " 39.3k/39.3k [00:00&lt;00:00, 3.02MB/s]"
          }
        },
        "f25b83cf9de94f8090585a851a32bda8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "840d2221d81f41828e03d409dde6c406": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e94cf58398484113a0e9b2cf7eec5263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b153ffbe8e5b43dab785b5db14fb98e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b7be4f4c7bf8436e81e161ce5b227346": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "56f8dd0b31c3479d99b745c193a93db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d40f1dc1cb454cc2a8409c9f06655f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d80e2f6b67a64175a4c44bd81cfb5d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fee4a54c2ee94fe1beb88fed3b347b6c",
              "IPY_MODEL_cf7986d1277b4689af19aa0bd340162c",
              "IPY_MODEL_1ed95d229c544bd48c8aab9c7d94cbc4"
            ],
            "layout": "IPY_MODEL_fc623c4097674bd6a8a98f3b88123263"
          }
        },
        "fee4a54c2ee94fe1beb88fed3b347b6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7a7b48e31088484e8940937d350d808a",
            "placeholder": "​",
            "style": "IPY_MODEL_51eda076d73540f48f325c2db81c6d92",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "cf7986d1277b4689af19aa0bd340162c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0045cf87262244bd8f489855b60c9feb",
            "max": 90888945,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7043a50ac9af477b824acd9cffedd764",
            "value": 90888945
          }
        },
        "1ed95d229c544bd48c8aab9c7d94cbc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f3488c4fa044f6397437059b1217876",
            "placeholder": "​",
            "style": "IPY_MODEL_0df574063501488a80196ccacbba6f2b",
            "value": " 90.9M/90.9M [00:00&lt;00:00, 153MB/s]"
          }
        },
        "fc623c4097674bd6a8a98f3b88123263": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7b48e31088484e8940937d350d808a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "51eda076d73540f48f325c2db81c6d92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0045cf87262244bd8f489855b60c9feb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7043a50ac9af477b824acd9cffedd764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f3488c4fa044f6397437059b1217876": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0df574063501488a80196ccacbba6f2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "239b13fd640b47c48a37fabe329d439f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a5bce8245c3249089552832adedfa59c",
              "IPY_MODEL_33ab83e6e0744ff7853bb43868b15d0e",
              "IPY_MODEL_f3cd703267334dd4a176a27f0e917a63"
            ],
            "layout": "IPY_MODEL_9088cabd91de4e0b9b53f2ddb7a2c32b"
          }
        },
        "a5bce8245c3249089552832adedfa59c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfca82b29f0845d5ab94d6e759a2e328",
            "placeholder": "​",
            "style": "IPY_MODEL_c255757eab294c0e8af314505071fd02",
            "value": "sentence_bert_config.json: 100%"
          }
        },
        "33ab83e6e0744ff7853bb43868b15d0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bd93b6ab2904aaa9eac5af25b8ed8e0",
            "max": 53,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0f68a5134e4140bf933fe696bb46a085",
            "value": 53
          }
        },
        "f3cd703267334dd4a176a27f0e917a63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b3c24c36abb422f8ca796408399ef1d",
            "placeholder": "​",
            "style": "IPY_MODEL_16a47b599c034591970b121b86d41a12",
            "value": " 53.0/53.0 [00:00&lt;00:00, 4.73kB/s]"
          }
        },
        "9088cabd91de4e0b9b53f2ddb7a2c32b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfca82b29f0845d5ab94d6e759a2e328": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c255757eab294c0e8af314505071fd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bd93b6ab2904aaa9eac5af25b8ed8e0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0f68a5134e4140bf933fe696bb46a085": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b3c24c36abb422f8ca796408399ef1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16a47b599c034591970b121b86d41a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e898c85a5ae14c73b5fa8acb046e57d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94733e2ac50b460780c111eb7658b83e",
              "IPY_MODEL_d30bc4a295ae4ad1a1ece802e3e81034",
              "IPY_MODEL_ba5f5e49d0a4476f9b0addb128ac60fa"
            ],
            "layout": "IPY_MODEL_1144fb80f7bd4fe5a829cb8bcb565df0"
          }
        },
        "94733e2ac50b460780c111eb7658b83e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e75a422c86764c1785f6f50292c31893",
            "placeholder": "​",
            "style": "IPY_MODEL_4bd106ff719f470bbbfb0bf3d4dba486",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "d30bc4a295ae4ad1a1ece802e3e81034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23fe8e7ba55643e4b3116e3c32e460c3",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5a856cfd54c7491ea3c1762c325f8763",
            "value": 112
          }
        },
        "ba5f5e49d0a4476f9b0addb128ac60fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f29ac1faf2f4aab8798ded3a83dc0c4",
            "placeholder": "​",
            "style": "IPY_MODEL_20f775aee1b341cca1fbcd498cff3295",
            "value": " 112/112 [00:00&lt;00:00, 9.78kB/s]"
          }
        },
        "1144fb80f7bd4fe5a829cb8bcb565df0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e75a422c86764c1785f6f50292c31893": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4bd106ff719f470bbbfb0bf3d4dba486": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "23fe8e7ba55643e4b3116e3c32e460c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a856cfd54c7491ea3c1762c325f8763": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8f29ac1faf2f4aab8798ded3a83dc0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20f775aee1b341cca1fbcd498cff3295": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dfcba1de4b124bea80110c415e5aa68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ab2ddc270d804867a16805ca50f0ecfb",
              "IPY_MODEL_c589964ae1024278a50031df38e3ff09",
              "IPY_MODEL_684049019c114d59b32ada9c7ea1d426"
            ],
            "layout": "IPY_MODEL_92c70d2a6f0a4cec81578f4af0f26401"
          }
        },
        "ab2ddc270d804867a16805ca50f0ecfb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb61efe564014579acdfce1b0af3492a",
            "placeholder": "​",
            "style": "IPY_MODEL_94baa031f525401487db92be4f5b2a3d",
            "value": "tokenizer.json: 100%"
          }
        },
        "c589964ae1024278a50031df38e3ff09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2e791b3b78874c8eb627acc01206d707",
            "max": 466247,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5b15660ba9d44f298f6e3ef9dd1ea979",
            "value": 466247
          }
        },
        "684049019c114d59b32ada9c7ea1d426": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e0188d44ca53485cbe0904b040b52de4",
            "placeholder": "​",
            "style": "IPY_MODEL_6fab13ab7e624ec9a1084956a647cdbe",
            "value": " 466k/466k [00:00&lt;00:00, 7.60MB/s]"
          }
        },
        "92c70d2a6f0a4cec81578f4af0f26401": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb61efe564014579acdfce1b0af3492a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94baa031f525401487db92be4f5b2a3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2e791b3b78874c8eb627acc01206d707": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b15660ba9d44f298f6e3ef9dd1ea979": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e0188d44ca53485cbe0904b040b52de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fab13ab7e624ec9a1084956a647cdbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fa52b4de77a543528daac7bd94ed18fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1e98e622dd9547058327189de5509c42",
              "IPY_MODEL_c2b704e6a0644aa49d34496c9cac8249",
              "IPY_MODEL_c56448b81e474dc9937bca5db8f901ab"
            ],
            "layout": "IPY_MODEL_2e1c78a7b1d54ac38f95d3e37a7a2554"
          }
        },
        "1e98e622dd9547058327189de5509c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_913b19e7619745babae69ff4e66e14b3",
            "placeholder": "​",
            "style": "IPY_MODEL_2fc4dfbac4034be0a5860589ad54e4ae",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c2b704e6a0644aa49d34496c9cac8249": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95fb6860c544c4ca53ba152a63a755c",
            "max": 350,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_824b395c752a48cfa25eca5378ddeea9",
            "value": 350
          }
        },
        "c56448b81e474dc9937bca5db8f901ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbe164990a3441bb9be622a8d71cea01",
            "placeholder": "​",
            "style": "IPY_MODEL_3541861f76aa454ab4934bc246a22573",
            "value": " 350/350 [00:00&lt;00:00, 31.8kB/s]"
          }
        },
        "2e1c78a7b1d54ac38f95d3e37a7a2554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "913b19e7619745babae69ff4e66e14b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2fc4dfbac4034be0a5860589ad54e4ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b95fb6860c544c4ca53ba152a63a755c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "824b395c752a48cfa25eca5378ddeea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dbe164990a3441bb9be622a8d71cea01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3541861f76aa454ab4934bc246a22573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bfa87df355b44111b531681cb5444a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e6dc7b841b7249e7829f8bd1f3aded3c",
              "IPY_MODEL_c3a013fb28d24da1a1abdb237a6333cc",
              "IPY_MODEL_44e6f62b35344facaa0fedc4f226c5cd"
            ],
            "layout": "IPY_MODEL_c6e6b4626e1c46c0b5cc76ac06200eb3"
          }
        },
        "e6dc7b841b7249e7829f8bd1f3aded3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f4026f51c384d3985b8e788a0df638d",
            "placeholder": "​",
            "style": "IPY_MODEL_fe9548efa1df4183b2545a183081fe23",
            "value": "train_script.py: 100%"
          }
        },
        "c3a013fb28d24da1a1abdb237a6333cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6acc7c4913a84c89ad8e3179e6d13003",
            "max": 13156,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6ba7ce45df2e454cb86fe5b77693fc5c",
            "value": 13156
          }
        },
        "44e6f62b35344facaa0fedc4f226c5cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c649fc92198c4fb28a7a38d97cb8ea11",
            "placeholder": "​",
            "style": "IPY_MODEL_ad322e7feb094bab8239d65b2efc4ae8",
            "value": " 13.2k/13.2k [00:00&lt;00:00, 672kB/s]"
          }
        },
        "c6e6b4626e1c46c0b5cc76ac06200eb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f4026f51c384d3985b8e788a0df638d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe9548efa1df4183b2545a183081fe23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6acc7c4913a84c89ad8e3179e6d13003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba7ce45df2e454cb86fe5b77693fc5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c649fc92198c4fb28a7a38d97cb8ea11": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad322e7feb094bab8239d65b2efc4ae8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c54d8ce7dcef40068ba74f8cf267155b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4a82530ab9324be1856ea6d51e875717",
              "IPY_MODEL_e127d73b00c343e4b41d7328c6cfc3e5",
              "IPY_MODEL_06d7bb46ee294cdaadcf45ed50eca1a3"
            ],
            "layout": "IPY_MODEL_d29e11a116a8488a82aed778f74f0101"
          }
        },
        "4a82530ab9324be1856ea6d51e875717": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49f51550ebe44b628ff1dd3f767a4314",
            "placeholder": "​",
            "style": "IPY_MODEL_3f88265860f4492ebf9f377723ce6ffb",
            "value": "vocab.txt: 100%"
          }
        },
        "e127d73b00c343e4b41d7328c6cfc3e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aee8eebd1b654e63871e257906881093",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_15c0507971f74ae8ba377853a883f069",
            "value": 231508
          }
        },
        "06d7bb46ee294cdaadcf45ed50eca1a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76d09613b0854d9a9bf6f98eb418404d",
            "placeholder": "​",
            "style": "IPY_MODEL_c37e93c82ee042fe81d0eaa0be65e5ab",
            "value": " 232k/232k [00:00&lt;00:00, 18.0MB/s]"
          }
        },
        "d29e11a116a8488a82aed778f74f0101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49f51550ebe44b628ff1dd3f767a4314": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f88265860f4492ebf9f377723ce6ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aee8eebd1b654e63871e257906881093": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15c0507971f74ae8ba377853a883f069": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76d09613b0854d9a9bf6f98eb418404d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37e93c82ee042fe81d0eaa0be65e5ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0dda4ebb5f2f4703a0072c950f369b4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_30b9795471574190af4582495666d887",
              "IPY_MODEL_d94f483318534126bc43380ca547bd79",
              "IPY_MODEL_bf7f6d22e7204774ba5eef3bd909cdd0"
            ],
            "layout": "IPY_MODEL_b039b1575fe74262a8a316d638d0ea00"
          }
        },
        "30b9795471574190af4582495666d887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4813310c4804b28b5f815fc78846e4c",
            "placeholder": "​",
            "style": "IPY_MODEL_605adad882414d05ae0169bc4b5a751b",
            "value": "modules.json: 100%"
          }
        },
        "d94f483318534126bc43380ca547bd79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c9d3634c634457f983c01e64d6fa29d",
            "max": 349,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ad08b5cb43b44421bff4475f850fae42",
            "value": 349
          }
        },
        "bf7f6d22e7204774ba5eef3bd909cdd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e49bf4f991b47f3ade4440970b0ac85",
            "placeholder": "​",
            "style": "IPY_MODEL_1a0dc50b55e547ad98e9b29c61ebe81d",
            "value": " 349/349 [00:00&lt;00:00, 29.9kB/s]"
          }
        },
        "b039b1575fe74262a8a316d638d0ea00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4813310c4804b28b5f815fc78846e4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "605adad882414d05ae0169bc4b5a751b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6c9d3634c634457f983c01e64d6fa29d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad08b5cb43b44421bff4475f850fae42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8e49bf4f991b47f3ade4440970b0ac85": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0dc50b55e547ad98e9b29c61ebe81d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}